{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURRENT NEURAL NETWORKS\n",
    "\n",
    "A limitation of vanilla neural networks is that they are constrained: they accept a fixed-sized vector as input and produce a fixed-sized vector as output. Recurrent networks allow us to operate over sequences of vectors of arbitrary sizes.\n",
    "\n",
    "![RNN](img/recurrent_nets.png \"RNN\")\n",
    "\n",
    "At each timestep, there are two inputs to the hidden layer: the output of the previous layer $ h_{t-1} $, and the input at that timestep $ x_{t} $. The former is multiplied by a weight matrix $ W_{h} $, and the latter by a weight matrix $ W_{x} $. The result is then run through a non-linearity function to produce the output at the current timestep $ h_{t} $.\n",
    "\n",
    "$$ h^{(t)} = tanh(W_{h}h^{(t-1)} + W_{x}x^{(t)} + b) $$\n",
    "\n",
    "It is important to note that the same weights $ W_{x} $ and $ W_{h} $ are applied repeatedly at each timestep. Thus, the number of parameters the model has to learn is less, and most importantly, is independent of the length of the input sequence.  \n",
    "At each timestep we could optionally produce an output using the weight matrix $ W_{o} $.\n",
    "\n",
    "The initial hidden state $ h^{(0)} $ can be learned as a parameter of the network or it can be assumed to be 0. In some problems the initial hidden state is given as an input to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from recurrent_net import *\n",
    "from solver import *\n",
    "from layers import *\n",
    "from utils.gradient_check import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the network we must compute the output distribution $ \\hat{y}^{(t)} $ for every timestep $ t $. After that we compute the loss at every timestep as the cross-entropy between the predicted probability distribution $ \\hat{y^{(t)}} $ and the true distribution. Finally, we average this over the entire training batch to get the overall loss.\n",
    "\n",
    "![RNN Training](img/recurrent_net_training.png \"RNN Training\")\n",
    "\n",
    "One problem with recurrent neural networks is the vanishing gradient. During backpropagation, the contribution of gradient values gradually vanishes as they propagate to earlier timesteps. Thus, for deep networks, long-term effects are not accounted for. Due to vanishing gradients, we don't know whether there is no dependency between steps $ t $ and $ t+n $ in the data, or we just cannot capture the true dependency due to this issue.  \n",
    "It is too difficult for the RNN to learn to perserve information over many timesteps.\n",
    "\n",
    "![RNN Gradient](img/recurrent_net_gradient.png \"RNN Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize toy example to check the implementation.\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "timesteps = 3\n",
    "hidden_dim = 6\n",
    "input_dim = 20\n",
    "output_dim = 20\n",
    "\n",
    "X = np.random.randn(batch_size, timesteps, input_dim)\n",
    "y = np.random.randint(low=0, high=10, size=(batch_size, timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 3.237110e-04\n",
      "Wh_0 relative error: 2.520765e-02\n",
      "Wx_0 relative error: 5.496978e-05\n",
      "b_0 relative error: 7.919711e-07\n",
      "b_out relative error: 4.360299e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for a vanilla RNN cell.\n",
    "rnn_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                             n_layers=1, cell_type=\"rnn\", dtype=np.float64)\n",
    "\n",
    "loss, grads = rnn_model.loss(X, y)\n",
    "f = lambda _ : rnn_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, rnn_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory\n",
    "\n",
    "In 1997 Sepp Hochreiter and Jurgen Schmidhuber proposed a solution to the vanishing gradient problem in their paper:  \n",
    "<i>[1] \"Long Short-Term Memory\"</i>.\n",
    "\n",
    "In standard RNNs at every timestep a simple non-linearity is applied to the output of the previous layer $ h^{(t-1)} $ and the input at that timestep $ x^{(t)} $.\n",
    "\n",
    "In LSTMs at every timestep there is a hidden state $ h^{(t)} $ and a <b>cell state</b> $ c^{(t)} $. The idea of the cell state is to store long-term information. During the forward pass the LSTM can <i>erase</i>, <i>write</i> and <i>read</i> information from the cell state. The selection of which information is erased/written/read is controlled by three gates based on the previus inputs. Gates are a way to optionally let information through. They are composed of a sigmoid function that outputs numbers between zero and one, describing how much of each component should be let through.\n",
    "\n",
    "![RNN LSTM](img/recurrent_net_lstm.png \"RNN LSTM\")\n",
    "\n",
    "#### Forget Gate\n",
    "\n",
    "The desicion what information to throw away from the cell state is made by a sigmoid layer called the forget gate. At timestep $ t $ it \"looks\" at $ h^{(t-1)} $ and $ x^{(t)} $, and outputs a number between zero and one for each number in the cell state $ c^{(t-1)} $.\n",
    "\n",
    "$$ f_{t} = \\sigma \\space (W_{fh}h^{(t-1)} + W_{fx}x^{(t)} + b_{f}) $$\n",
    "\n",
    "#### Input Gate\n",
    "\n",
    "The next step is to decide what new information to store in the cell state. This operation has two parts:\n",
    " * a <i>tanh</i> layer creates a vector of new candidate values\n",
    " * a sigmoid layer called the input gate decides which values we will store\n",
    "\n",
    "$$ i_{t} = \\sigma \\space (W_{ih}h^{(t-1)} + W_{ix}x^{(t)} + b_{i}) $$\n",
    "$$ g_{t} = \\tanh \\space (W_{gh}h^{(t-1)} + W_{gx}x^{(t)} + b_{f}) $$\n",
    "\n",
    "The new cell state is calculated by multiplying the old state by $ f_{t} $ and adding the candidate values multiplied by $ i_{t} $:\n",
    "\n",
    "$$ c^{(t)} = f_{t} * c^{(t-1)} + i_{t} * g_{t} $$\n",
    "\n",
    "#### Output Gate\n",
    "\n",
    "Finally, the output gate decides what information we are going to output. The output is a filtered version of the current cell state and the output gate is again a sigmoid layer:\n",
    "\n",
    "$$ o_{t} = \\tanh \\space (W_{oh}h^{(t-1)} + W_{ox}x^{(t)} + b_{o}) $$\n",
    "$$ h^{t} = o_{t} * tanh(c^{t}) $$\n",
    "\n",
    "![RNN LSTM-Detailed](img/recurrent_net_lstm_detailed.png \"RNN LSTM Detailed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 6.435300e-05\n",
      "Wh_0 relative error: 1.051250e-01\n",
      "Wx_0 relative error: 1.426676e-01\n",
      "b_0 relative error: 6.363907e-02\n",
      "b_out relative error: 2.647290e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for an LSTM cell.\n",
    "lstm_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                              n_layers=1, cell_type=\"lstm\", dtype=np.float64)\n",
    "\n",
    "loss, grads = lstm_model.loss(X, y)\n",
    "f = lambda _ : lstm_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, lstm_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer RNNs\n",
    "\n",
    "One way to create more complex RNN models is to stack the networks in layers.  \n",
    "The first layer of the recurrent network reads the input and produces a sequence of hidden states. Now we could use this sequence of hidden states as an input to another recurrent network. We could stack layers in this manner to produce a multilayer RNN.\n",
    "\n",
    "![RNN Multilayer](img/recurrent_net_multilayer.png \"RNN Multilayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out relative error: 4.147952e-03\n",
      "Wh_0 relative error: 2.675589e-07\n",
      "Wh_1 relative error: 4.386354e-07\n",
      "Wh_2 relative error: 7.955481e-07\n",
      "Wx_0 relative error: 7.590113e-03\n",
      "Wx_1 relative error: 2.964636e-03\n",
      "Wx_2 relative error: 6.034230e-03\n",
      "b_0 relative error: 6.084254e-03\n",
      "b_1 relative error: 8.277030e-03\n",
      "b_2 relative error: 2.925164e-06\n",
      "b_out relative error: 7.677432e-10\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass for a multilayer RNN.\n",
    "multilayer_lstm_model = RecurrentNetwork(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                                         n_layers=3, cell_type=\"lstm\", dtype=np.float64)\n",
    "\n",
    "loss, grads = multilayer_lstm_model.loss(X, y)\n",
    "f = lambda _ : multilayer_lstm_model.loss(X, y)[0]\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    param_grad_num = eval_numerical_gradient(f, multilayer_lstm_model.params[param_name], verbose=False, h=1e-6)\n",
    "    print('%s relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level Language Model\n",
    "\n",
    "We will train a character level language model using an LSTM recurrent network.  \n",
    "To train the model we wiil use a simple text file. We will encode every character into a one-hot vector encoding, and feed the network one character at a time. The size of our vocabulary will be the number of distinct characters in the dataset. At each time-step the output of the network will be a vector of dimension `vocab_size`, which we will interpret as the score the RNN assigns to each character comming next in the sequence.\n",
    "\n",
    "![RNN Chars](img/recurrent_net_chars.png \"RNN Chars\")\n",
    "\n",
    "During training, for subsequent inputs we will use the actual ground truth next character in the sequence and we stop generating characters once we generate the target length. Once we have our predicted target sequence, we compare it against our actual target sequence to calculate the loss.  \n",
    "During inference, every predicted character by the model we will feed as the next input and we keep generating characters until a certain amount of characters have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 442769 symbols\n",
      "Example text:\n",
      "\n",
      " Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "filename = \"datasets/text/HarryPotter_book1.txt\"\n",
    "with open(filename, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Length of the text: %d symbols\" % len(text))\n",
    "print(\"Example text:\\n\\n\", text[:336])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct characters: 82\n",
      "Vocab:\n",
      " {'A': 0, '2': 1, 'r': 2, '4': 3, 'n': 4, 'I': 5, '8': 6, 'D': 7, 'P': 8, \"'\": 9, '!': 10, '6': 11, '(': 12, 'N': 13, 'G': 14, 'e': 15, 'q': 16, 'h': 17, 'X': 18, ';': 19, 'i': 20, ')': 21, 'c': 22, 'O': 23, 'U': 24, 'F': 25, '\"': 26, 'M': 27, 'H': 28, 'Z': 29, '*': 30, ':': 31, 'b': 32, 'x': 33, '\\n': 34, 'K': 35, 'f': 36, 'Y': 37, '.': 38, 'd': 39, 'T': 40, 't': 41, 'v': 42, 'o': 43, '0': 44, 'g': 45, '?': 46, '7': 47, '\\\\': 48, 'z': 49, 'p': 50, 's': 51, 'S': 52, 'y': 53, 'R': 54, '-': 55, ',': 56, 'Q': 57, 'E': 58, ' ': 59, 'œ': 60, '3': 61, 'â': 62, '~': 63, 'w': 64, 'L': 65, 'a': 66, '€': 67, 'V': 68, '9': 69, 'B': 70, '5': 71, 'j': 72, 'C': 73, 'l': 74, '“': 75, '1': 76, 'J': 77, 'm': 78, 'k': 79, 'W': 80, 'u': 81}\n",
      "Example text after preprocessing:\n",
      " [28, 66, 2, 2, 53, 59, 8, 43, 41, 41, 15, 2, 59, 66, 4, 39, 59, 41, 17, 15, 59, 52, 43, 2, 22, 15, 2, 15, 2, 9]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to create char-to-int and int-to-char mappings.\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i,ch in enumerate(chars)}\n",
    "\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "print(\"Number of distinct characters: %d\" % vocab_size)\n",
    "print(\"Vocab:\\n\", char_to_idx)\n",
    "\n",
    "print(\"Example text after preprocessing:\\n\", data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(object):\n",
    "    def __init__(self, data, seq_length):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - data: List of integers representing the preprocessed data.\n",
    "          Every integer corresponds to a character.\n",
    "        - seq_length: Integer giving the length of the sequence to be input\n",
    "          to the recurrent neural network.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate the next batch of examples from the data.\n",
    "\n",
    "        Returns:\n",
    "        - batch: A numpy array of integers of shape (batch_size, seq_length) giving\n",
    "          a batch of training examples.\n",
    "        \"\"\"\n",
    "        seq_length = self.seq_length\n",
    "        batch = np.ndarray((batch_size, seq_length), dtype=np.int)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            cursor = np.random.randint(len(self.data)-seq_length)\n",
    "            batch[idx] = self.data[cursor : cursor + seq_length]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def num_train(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - num_train: Integer, giving the number of training examples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelRNN(object):\n",
    "    def __init__(self, vocab_size, hidden_dim, char_to_idx,\n",
    "                 n_layers=1, reg=0.0, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - vocab_size: Integer giving the size of the vocabulary.\n",
    "        - hidden_dim: Integer giving the hidden size.\n",
    "        - n_layer: Integer giving the number of recurrent layers to use.\n",
    "        - dtype: Numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "        self.RNN = RecurrentNetwork(input_dim=vocab_size,\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    output_dim=vocab_size,\n",
    "                                    n_layers=n_layers,\n",
    "                                    weight_scale=1,\n",
    "                                    reg=reg,\n",
    "                                    cell_type=\"lstm\",\n",
    "                                    dtype=dtype)\n",
    "\n",
    "        self.params = self.RNN.params\n",
    "\n",
    "    def one_hot_encoding(self, x, V):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: A numpy array of shape (N, T) giving a batch of integer indices.\n",
    "        \n",
    "        Returns:\n",
    "        - embed: A numpy array of shape (N, T vocab_size) giving one-hot encodings\n",
    "          of each element.\n",
    "        \"\"\"\n",
    "        x = x.astype(np.int)\n",
    "\n",
    "        if len(x.shape) == 1: # if T = 1\n",
    "            N = x.shape[0]\n",
    "            embed = np.zeros((N, V), dtype=np.int)\n",
    "            embed[np.arange(N), x] = 1\n",
    "        else:\n",
    "            N, T = x.shape\n",
    "            embed = np.zeros((N, T, V), dtype=np.int)\n",
    "            embed[np.array([np.arange(N)]*(T)).transpose(1, 0), [np.arange(T)]*N, x] = 1\n",
    "\n",
    "        return embed\n",
    "\n",
    "    def loss(self, X):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, T) of integers giving the input sequence\n",
    "          to the RNN. Each element is in the range 0 <= X[i, j] < V.\n",
    "  \n",
    "        Returns:\n",
    "        - loss: A scalar value giving the loss.\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        embed_x = self.one_hot_encoding(X, self.vocab_size)\n",
    "        embed_in = embed_x[:, : -1]\n",
    "        y = X[:, 1 :]\n",
    "        loss, grads = self.RNN.loss(embed_in, y)\n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "    def sample(self, start=None, max_length=100):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - start: A numpy array of shape (N, ) of integers giving input data.\n",
    "          If None, the sequence starts with \" \".\n",
    "        - max_length: Maximum length T of generated outputs.\n",
    "\n",
    "        Returns:\n",
    "        - sequence: Array of shape (N, max_length) giving sampled outputs.\n",
    "        \"\"\"\n",
    "        if start is None:\n",
    "            start = np.array([np.random.randint(self.vocab_size)])\n",
    "\n",
    "        N = start.shape[0]\n",
    "        start_embed = self.one_hot_encoding(start, self.vocab_size)\n",
    "        output = self.RNN.sample(start_embed, self.one_hot_encoding, max_length=max_length)\n",
    "        data = np.argmax(output, axis=2)\n",
    "        sequence = [[idx_to_char[idx] for idx in seq] for seq in data]\n",
    "        text = [''.join(seq) for seq in sequence]\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "batch_size = 128\n",
    "\n",
    "# Initialize the dataset.\n",
    "dataset = text_dataset(data, seq_length=seq_length)\n",
    "\n",
    "# Initialize the model.\n",
    "np.random.seed(seed=None)\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "reg = 1e-2\n",
    "\n",
    "char_level_model = CharLevelRNN(vocab_size=vocab_size,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                char_to_idx=char_to_idx,\n",
    "                                n_layers=n_layers,\n",
    "                                reg=reg,\n",
    "                                dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations per epoch: 3459\n",
      "(Iteration 0 / 3459); Epoch(0 / 1); loss: 48.45378\n",
      "Sample:\n",
      " 1 the did the did Harry sard sard sard sard sard sard sard sard sard sard sard sard sard sard sard s\n",
      "(Iteration 500 / 3459); Epoch(0 / 1); loss: 47.45308\n",
      "Sample:\n",
      " t and the was and the was and the was and the was and the was and the was and the was and the was an\n",
      "(Iteration 1000 / 3459); Epoch(0 / 1); loss: 45.36169\n",
      "Sample:\n",
      " ) the was a for the was a for the was a for the was a for the was a for the was a for the was a for \n",
      "(Iteration 1500 / 3459); Epoch(0 / 1); loss: 44.93584\n",
      "Sample:\n",
      " Quid the store the store the store the store the store the store the store the store the store the s\n",
      "(Iteration 2000 / 3459); Epoch(0 / 1); loss: 44.17052\n",
      "Sample:\n",
      " œ the said and the for the for the for the for the for the for the for the for the for the for the f\n",
      "(Iteration 2500 / 3459); Epoch(0 / 1); loss: 44.95854\n",
      "Sample:\n",
      " t the was the for the for the for the for the for the for the for the for the for the for the for th\n",
      "(Iteration 3000 / 3459); Epoch(0 / 1); loss: 43.46591\n",
      "Sample:\n",
      " Dudley said Harry sall the for the for the for the for the for the for the for the for the for the f\n",
      "training took 37.459 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "rnn_solver = UnsupervisedSolver(char_level_model, dataset,\n",
    "                                update_rule=\"adam\",\n",
    "                                optim_config={\"learning_rate\":1e-3},\n",
    "                                lr_decay=0.99,\n",
    "                                batch_size=batch_size,\n",
    "                                num_epochs=1,\n",
    "                                print_every=500,\n",
    "                                verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "rnn_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
