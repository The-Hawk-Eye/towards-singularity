{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH NORMALIZATION\n",
    "\n",
    "Batch Normaliaztion was introduced by Sergey Ioffe and Christian Szegedy in 2015 in their paper:  \n",
    "<i>[1] \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"</i>.\n",
    "\n",
    "In 2016 Jimmy Lei Ba, Jamie Ryan Kiros and Geoffrey E. Hinton address some problems of batch normalization in their paper:  \n",
    "<i>[2] \"Layer Normalization\"</i>.\n",
    "\n",
    "In 2018 Yuxin Wu and Kaiming He proposed an intermediary technique in their paper:  \n",
    "<i>[3] \"Group Normalization\"</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers import *\n",
    "from utils.gradient_check import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance.  \n",
    "When training a neural network, we usually preprocess the data before feeding it to the network to ensure that the first layer of the network sees data that follows a nice distribution. We could even go a step further and whiten the data to explicitly decorrelate its features. Network training converges faster if its inputs are whitened - i.e., linearly transformed to have zero means and unit variances, and decorrelated.  \n",
    "However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance, since they are output from earlier layers in the network. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonliearities.  \n",
    "During the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated. This phenomenon is referred to as <i>internal covariate shift</i>\n",
    "\n",
    "A network computing a loss function\n",
    "\n",
    "$$ l = F_{2}(F_{1}(x, \\theta_{1})), \\theta_{2}) $$\n",
    "\n",
    "can be viewed as if the inputs $ u = F_{1}(x, \\theta_{1}) $ are fed into a sub-network $ l = F_{2}(u, \\theta_{2}) $.\n",
    "\n",
    "Therefore, the input distribution properties that make training more efficient apply to training the sub-network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a number of training cycles on a neural net and plotting the activations\n",
    "# of each layer during each cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same form of preprocessing of the inputs of each layer. However, if these modifications are inserted during the forward pass, then during the backward pass the gradient descent optimization has to take into account the fact that the normalization takes place.\n",
    "\n",
    "Writing the normalization as a transformation: $ \\hat{u} = Norm(u, \\Psi) $, we see that for brackpopagation we need to compute the Jacobians:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\partial Norm(u, \\Psi)}{\\partial u} \\space \\text{and} \\space  \\frac{\\partial Norm(u, \\Psi)}{\\partial \\Psi} $$\n",
    "\n",
    "Whitin this framework, whitening the layer inputs is expensive as it requires computing the covariance matrix and its inverse square root, to produce the whitened activations, as well as the derivatives of these transforms for backpropagation. Thus, it is reasonable to perform input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrtie some code doing whitening and time it to check how slow it actually runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of whitening the layer inputs, we will normalize each feature by making it have zero mean and unit variance.\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\hat{u}^{(k)} = \\frac{u^{(k)} - E[u^{(k)}]}{\\sqrt{Var[u^{(k)}]}}\n",
    "$$\n",
    "\n",
    "To compute the gradient, we could write out a computation graph composed of simple operations and backpropagate through all intermediate values.\n",
    "\n",
    "![BatchNorm Graph](img/batchnorm_graph.png \"BatchNorm Graph\")\n",
    "\n",
    "However, simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, for each activation $ u^{(k)} $ , a pair of parameters $ \\gamma ^{(k)} $ and $ \\beta ^{(k)} $ are introduced, which scale and shift the normalzied value:\n",
    "\n",
    "$$ y^{(k)} = \\gamma ^{(k)} \\hat{u}^{(k)} + \\beta ^{(k)} $$\n",
    "\n",
    "These parameters are learned along with the original model parameters, and restore the representational power of the network. We could recover the original activations, if that were the optimal thing to do.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code to show why simply normalizing is not a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm\n",
    "\n",
    "To normalize the output of a given layer, we need to compute the mean and the variance of every feature in that layer.\n",
    "One way to do that is to compute the value of a given feature, $ u^{(k)} $, for every training example in the dataset $ \\{x_{i}\\}_{i=1}^{N} $ and then compute the mean and the variance over these values:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "& E[u^{(k)}] = \\frac{1}{N} \\sum_{i=1}^{N} u_{i}^{(k)} \\\\\n",
    "& Var[u^{(k)}] = \\frac{1}{N} \\sum_{i=1}^{N} (u_{i}^{(k)} - E[u^{(k)}])^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "![DataSet Norm](img/batchnorm_datasetnorm.png \"DataSet Norm\")\n",
    "\n",
    "However, this is impractical when using stochastic optimization with mini-batches. Instead of computing the value of every feature for every training example in the dataset, we will use only the values computed for that mini-batch. Thus, for a mini-batch of size $ m $ we have:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "& \\mu_{\\beta}^{(k)} = \\frac{1}{m} \\sum_{i=1}^{m} u_{i}^{(k)} \\\\\n",
    "& \\upsilon_{\\beta}^{(k)} = \\frac{1}{m} \\sum_{i=1}^{m} (u_{i}^{(k)} - \\mu_{\\beta}^{(k)})^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The values $ \\mu_{\\beta}^{(k)} $ and $ \\upsilon_{\\beta}^{(k)} $ give us <b><i>estimates</i></b> of the mean and the variance of the feature $ u^{(k)} $.  \n",
    "Thus, normalization for a given feature $ u^{(k)}$ takes the form:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "& \\hat{u}^{(k)} = \\frac{u^{(k)} - \\mu_{\\beta}^{(k)}}{\\sqrt{\\upsilon_{\\beta}^{(k)} - \\epsilon}} \\\\\n",
    "& y^{(k)} = \\gamma^{(k)} \\hat{u}^{(k)} + \\beta^{(k)} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "![BatchNorm](img/batchnorm.png \"BatchNorm\")\n",
    "\n",
    "Normalizing using mini-batch statistics allows efficient training, but a question arises during testing how to cumpute the mean and the variance. During testing the mean and the variance are not computed using mini-batch statistics, because we want the output of a single training example to depend only on the training example itself, and not on the entire batch. When normalizing during testing we need to use means and variances computed over the <b><u>training set</u></b>. One way is to calculate an estimate of the mean and the variance using population statistics:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "& E[u^{(k)}] = E_{\\beta}[\\mu_{\\beta}^{(k)}] \\\\\n",
    "& Var[u^{(k)}] = \\frac{m}{m -1} E_{\\beta}[\\upsilon_{\\beta}^{(k)}]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once the network has been trained, we compute the means and variances for a large number of mini-batches of size $ m $, and we average over these values. Note that the unbiased estimate for the variance is used.\n",
    "\n",
    "Using this approach, however, it is not very easy to keep track of the accuracy of the model during training. Another way for calculating the means and variances is to use <i>running averages</i>. For every feature, $ u^{(k)} $, we keep a running mean, $ \\mu_{run}^{(k)} $, and a running variance, $ \\upsilon_{run}^{(k)} $. At every training step we compute the sample mean, $ \\mu_{\\beta}^{(k)} $, and the sample variance, $ \\upsilon_{\\beta}^{(k)} $, for the feature $ u^{(k)} $, and we update these running averages using an exponantially decaying rule:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "& \\mu_{run}^{(k)} = p \\mu_{run}^{(k)} + (1 - p) \\mu_{\\beta}^{(k)} \\\\\n",
    "& \\upsilon_{run}^{(k)} = p \\upsilon_{run} + (1 - p) \\upsilon_{\\beta}^{(k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ p $ is a momentum parameter for decaying the running averages.\n",
    "\n",
    "During training we also need to propagate the gradient loss through this transformation. Assuming we have a mini-batch of training examples $ \\{x_{i}\\}_{i=1}^{m} $, and we have computed the loss $ \\frac{\\partial L}{\\partial y_{i}} $ for every $ i $, then we can compute the derivatives using chain rule. For a function of multiple variables we will use the generalized chain rule:\n",
    "\n",
    "<i>Let $ w = f(y_{1}, y_{2}, ..., y_{n}) $ be a differentiable function of $ n $ independent variables and, for each $ 1 \\leq i \\leq n $, let $ y_{i} = g(x_{1}, .., x_{m}) $ be a differentiable function of $ m $ independent variables. Then:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\partial w}{\\partial x_{i}}= \\frac{\\partial w}{\\partial y_{1}} \\frac{\\partial y_{1}}{x_{i}} + \\cdots + \\frac{\\partial w}{\\partial y_{n}} \\frac{\\partial y_{n}}{\\partial x_{i}}\n",
    "$$\n",
    "\n",
    "NOTE: For the function $ f(x, g(x)) $ we could apply the following trick: set $ h(x) = x $, then $ f(x, g(x)) = f(h(x), g(x)) $. For the derivative with respect to $ x $ we have:</i>\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\partial f}{dx} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{dx} + \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{dx}\n",
    "$$\n",
    "\n",
    "For the gradients with respect to the shift and scale parameters we have:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\gamma^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_{i}^{(k)}} \\frac{\\partial y_{i}^{(k)}}{\\partial \\gamma^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_{i}^{(k)}} \\hat{u}_{i}^{(k)}\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\beta^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_{i}^{(k)}} \\frac{\\partial y_{i}^{(k)}}{\\partial \\beta^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_{i}^{(k)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the gradient with respect to the input to the normalization transformation we have:\n",
    "$$ \\displaystyle\n",
    "\\begin{align*}\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial y_{i}^{(k)}} \\frac{\\partial y_{i}^{(k)}}{\\partial \\hat{u}_{i}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial y_{i}^{(k)}} \\gamma^{(k)}\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\sigma_{\\beta}^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\frac{\\partial \\hat{u}_{i}^{(k)}}{\\partial \\sigma_{\\beta}^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\left( \\frac{u_{i}^{(k)} - \\mu_{\\beta}^{(k)}}{(\\sigma_{\\beta}^{(k)}) ^{2}} \\right)\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\upsilon_{\\beta}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial \\sigma_{\\beta}^{(k)}} \\frac{\\partial \\sigma_{\\beta}^{(k)}}{\\partial \\upsilon_{\\beta}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial \\sigma_{\\beta}^{(k)}} \\left( -\\frac{1}{2 \\sigma_{\\beta}^{(k)}} \\right)\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial L}{\\partial \\mu_{\\beta}^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\frac{\\partial \\hat{u}_{i}^{(k)}}{\\partial \\mu_{\\beta}^{(k)}} + \\frac{\\partial L}{\\partial \\upsilon_{\\beta}^{(k)}} \\frac{\\partial \\upsilon_{\\beta}^{(k)}}{\\partial \\mu_{\\beta}^{(k)}}\n",
    "= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\left( -\\frac{1}{\\sigma_{\\beta}^{(k)}} \\right) - \\frac{\\partial L}{\\partial \\upsilon_{\\beta}^{(k)}}\\frac{2}{m} \\sum_{i=1}^{m}(u_{i}^{(k)} - \\mu_{\\beta}^{(k)})\n",
    "\\\\\n",
    "&\n",
    "\\frac{\\partial L}{\\partial u_{i}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\frac{\\partial \\hat{u}_{i}^{(k)}}{\\partial u_{i}^{(k)}} + \\frac{\\partial L}{\\partial \\mu_{\\beta}^{(k)}} \\frac{\\partial \\mu_{\\beta}^{(k)}}{u_{i}^{(k)}} + \\frac{\\partial L}{\\upsilon_{\\beta}^{(k)}} \\frac{\\upsilon_{\\beta}^{(k)}}{u_{i}^{(k)}}\n",
    "= \\frac{\\partial L}{\\partial \\hat{u}_{i}^{(k)}} \\frac{1}{\\sigma_{\\beta}^{(k)}} + \\frac{\\partial L}{\\partial \\mu_{\\beta}^{(k)}} \\frac{1}{m} + \\frac{\\partial L}{\\partial \\upsilon_{\\beta}^{(k)}} \\frac{2(u_{i}^{(k)} - \\mu_{\\beta}^{(k)})}{m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "After some simplifications we arrive at:\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\partial L}{\\partial u_{i}^{(k)}}\n",
    "= \\left[ \\frac{\\partial L}{\\partial y_{i}^{(k)}} - \\frac{1}{m} \\left( \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}^{(k)}} + \\hat{u}_{i}^{(k)}\\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}^{k}} \\hat{u}_{j}^{(k)} \\right) \\right] \\frac{\\gamma^{(k)}}{\\sigma^{(k)}}\n",
    "$$\n",
    "\n",
    "\n",
    "Applying Batch normalization ammounts to inserting BatchNorm layers immediately after fully-connected layers and before non-linearities.\n",
    "\n",
    "![BatchNorm Layer](img/batchnorm_layer.png \"BatchNorm Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer inputs after batch normalization: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the forward pass.\n",
    "# Initialize a nerual net.\n",
    "net_dims = [64, 256, 256, 256, 16]\n",
    "params = {}\n",
    "bn_params = []\n",
    "for i in range(len(net_dims) - 1):\n",
    "    params[\"W%d\" % (i + 1)] = np.random.randn(net_dims[i], net_dims[i + 1]) * 1e-3\n",
    "    params[\"b%d\" % (i + 1)] = np.zeros(net_dims[i + 1])\n",
    "    params[\"gamma%d\" % (i + 1)] = np.ones(net_dims[i + 1])\n",
    "    params[\"beta%d\" % (i + 1)] = np.zeros(net_dims[i + 1])\n",
    "    bn_params.append({\"mode\": \"train\"})\n",
    "\n",
    "\n",
    "# Run the training-time forward pass many times to warm up the running averages.\n",
    "num_examples = 16\n",
    "for t in range(1024):\n",
    "    in_data = np.random.randn(num_examples, net_dims[0])\n",
    "    for i in range(len(net_dims) - 1):\n",
    "        in_data, _ = affine_forward(in_data, params[\"W%d\" % (i + 1)],\n",
    "                                    params[\"b%d\" % (i + 1)])\n",
    "        in_data, _ = batchnorm_forward(in_data, params[\"gamma%d\" % (i + 1)],\n",
    "                                       params[\"beta%d\" % (i + 1)],\n",
    "                                       bn_params[i])\n",
    "        in_data, _ = relu_forward(in_data)\n",
    "\n",
    "\n",
    "# Check the means and variances of activations after a test-time forward pass.\n",
    "for i in range(len(net_dims) - 1):\n",
    "    bn_params[i][\"mode\"] = \"test\"\n",
    "layers = {}\n",
    "in_data = np.random.randn(num_examples, net_dims[0])\n",
    "for i in range(len(net_dims) - 1):\n",
    "    in_data, _ = affine_forward(in_data, params[\"W%d\" % (i + 1)],\n",
    "                                params[\"b%d\" % (i + 1)])\n",
    "    layers[\"%d\" % (i + 1)], _ = batchnorm_forward(in_data, params[\"gamma%d\" % (i + 1)],\n",
    "                                                  params[\"beta%d\" % (i + 1)],\n",
    "                                                  bn_params[i])\n",
    "    in_data, _ = relu_forward(layers[\"%d\" % (i + 1)])\n",
    "\n",
    "\n",
    "# Plot the activations.\n",
    "# Means should be close to zero and stds close to one.\n",
    "print(\"Layer inputs after batch normalization: \")\n",
    "plt.subplots(figsize=(16.0, 3.0))\n",
    "for i in range(len(net_dims) - 2):\n",
    "    mean = np.mean(layers[\"%d\" % (i + 1)])\n",
    "    std = np.std(layers[\"%d\" % (i + 1)])\n",
    "    plt.subplot(1, len(net_dims) - 2, i + 1)\n",
    "    plt.hist(layers[\"%d\" % (i + 1)][0])\n",
    "    plt.title(\"layer %d \\nmean = %.3f \\nstd = %.4f\" % (i + 1, mean, std))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 1.217850e-06\n",
      "gamma max relative error: 2.574662e-09\n",
      "beta max relative error: 2.237326e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass.\n",
    "# Initialize toy data.\n",
    "D = 32\n",
    "num_classes = 10\n",
    "x = np.random.randn(num_examples, D)\n",
    "y = np.random.randint(low=1, high=num_classes, size=num_examples)\n",
    "\n",
    "# Initialize the weights\n",
    "W = np.random.randn(D, num_classes)\n",
    "b = np.random.randn(num_classes)\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "params = {\"x\":x, \"gamma\":gamma, \"beta\":beta}\n",
    "\n",
    "\n",
    "# Using numeric gradient checking to check the implementation of the backward pass.\n",
    "# If the implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of the model parameters.\n",
    "norm_out, norm_cache = batchnorm_forward(x, gamma, beta, bn_param={\"mode\": \"train\"})\n",
    "out, cache = affine_forward(norm_out, W, b)\n",
    "_, dout = cross_entropy_loss(out, y)\n",
    "dout, _, _ = affine_backward(dout, cache)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, norm_cache)\n",
    "grads = {\"x\":dx, \"gamma\":dgamma, \"beta\":dbeta}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: cross_entropy_loss(affine_forward(batchnorm_forward(x, gamma, beta, bn_param={\"mode\": \"train\"})[0],\n",
    "                                          W, b)[0], y)[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Batch Norm\n",
    "\n",
    "Batch normalization can also be used for convolutional neural networks, but it needs to be tweaked a bit. This modification might be called <i>Spatial Batch Normalization</i>.  \n",
    "Normally, BatchNorm layers accept inputs of shape $ (m, D) $, where $ m $ is the mini-batch size, and $ D $ is the number of features, and the normalization is done accross all examples in the mini-batch. In conv nets BatchNorm layers need to accept inputs of shape $ (m, C, H, W) $, where $ C $ is the number of channels, and $ H $ and $ W $ give the spatial dimensions of the feature map. We want different elements of the same feature map, at different locations, to be normalized in the same way. Thus, we will compute a mean and variance for every channel $ C $ by computing statistics over both the elements of the mini-batch and the spatial locations. Effectively, we use a mini-batch of size $ m' = mHW $.  \n",
    "The reason we compute the mean and the variance in this ways is because we expect means and variances computed at different locations of the same feature map to be relatively consistent between different image inputs. This is because every location of the feature map is produced by the same convolutional filter.\n",
    "\n",
    "![Spatial BatchNorm](img/batchnorm_spatial.png \"Spatial BatchNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some experiments to see that this is truly the case.\n",
    "# Namely, that means and variances computed at different locations of the same feature map\n",
    "# are relatively consistent between different image inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm\n",
    "\n",
    "BatchNorm exhibits drawbacks that are caused by its distinct behaviour of normalizing along the batch dimension. The effect of batch normalization greatly depends on the mini-batch size. Reducing the batch size leads to inaccurate estimation of the batch statistics and this in turn increases the model error dramatically. This dependancy makes it less useful in complex networks which have a cap on the input batch size due to hardware limitations.  \n",
    "Another problem is that BatchNorm requires keeping running averages in order to run a forward pass at test-time. In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, applying batch normalization to Recurrent Neural Networks requires computing sums of varying lengths depending the length of the sequence.\n",
    "\n",
    "A simple solution, called <i>Layer nomralization</i>, is proposed, that suggests computing the mean and variance over the hidden units in the same layer. Thus for a given layer we have:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "& \\mu_{L} = \\frac{1}{H} \\sum_{i=1}^{H} u_{i} \\\\\n",
    "& \\upsilon_{L} = \\frac{1}{H} \\sum_{i=1}^{H} (u_{i} - \\mu_{L})^{2}\n",
    "\\end{align*} $$\n",
    "\n",
    "In other words, instead of normalizing over the batch, we normalize over the neurons in the given layer. Unlike batch normalization, layer normalization does not impose any constraint on the size of a mini-batch and it can be used in the pure online regime with batch size of 1. Another difference is that we dont keep track of running averages and the testing phase is identical to the training phase.\n",
    "\n",
    "![LayerNorm](img/batchnorm_layernorm.png \"LayerNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer inputs after layer normalization: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the forward pass.\n",
    "# Initialize a nerual net.\n",
    "net_dims = [64, 256, 256, 256, 16]\n",
    "params = {}\n",
    "for i in range(len(net_dims) - 1):\n",
    "    params[\"W%d\" % (i + 1)] = np.random.randn(net_dims[i], net_dims[i + 1]) * 1e-3\n",
    "    params[\"b%d\" % (i + 1)] = np.zeros(net_dims[i + 1])\n",
    "    params[\"gamma%d\" % (i + 1)] = np.ones(net_dims[i + 1])\n",
    "    params[\"beta%d\" % (i + 1)] = np.zeros(net_dims[i + 1])\n",
    "\n",
    "\n",
    "# Check the means and variances of activations after a forward pass.\n",
    "layers = {}\n",
    "in_data = np.random.randn(num_examples, net_dims[0])\n",
    "for i in range(len(net_dims) - 1):\n",
    "    in_data, _ = affine_forward(in_data, params[\"W%d\" % (i + 1)],\n",
    "                                params[\"b%d\" % (i + 1)])\n",
    "    layers[\"%d\" % (i + 1)], _ = layernorm_forward(in_data, params[\"gamma%d\" % (i + 1)],\n",
    "                                                  params[\"beta%d\" % (i + 1)], {})\n",
    "    in_data, _ = relu_forward(layers[\"%d\" % (i + 1)])\n",
    "\n",
    "\n",
    "# Plot the activations.\n",
    "# Means should be close to zero and stds close to one.\n",
    "print(\"Layer inputs after layer normalization: \")\n",
    "plt.subplots(figsize=(16.0, 3.0))\n",
    "for i in range(len(net_dims) - 2):\n",
    "    mean = np.mean(layers[\"%d\" % (i + 1)])\n",
    "    std = np.std(layers[\"%d\" % (i + 1)])\n",
    "    plt.subplot(1, len(net_dims) - 2, i + 1)\n",
    "    plt.hist(layers[\"%d\" % (i + 1)][0])\n",
    "    plt.title(\"layer %d \\nmean = %.3f \\nstd = %.4f\" % (i + 1, mean, std))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x max relative error: 1.035731e-06\n",
      "gamma max relative error: 1.403842e-09\n",
      "beta max relative error: 9.737706e-10\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass.\n",
    "# Initialize toy data.\n",
    "D = 32\n",
    "num_classes = 10\n",
    "x = np.random.randn(num_examples, D)\n",
    "y = np.random.randint(low=1, high=num_classes, size=num_examples)\n",
    "\n",
    "# Initialize the weights\n",
    "W = np.random.randn(D, num_classes)\n",
    "b = np.random.randn(num_classes)\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "params = {\"x\":x, \"gamma\":gamma, \"beta\":beta}\n",
    "\n",
    "\n",
    "# Using numeric gradient checking to check the implementation of the backward pass.\n",
    "# If the implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of the model parameters.\n",
    "norm_out, norm_cache = layernorm_forward(x, gamma, beta, {})\n",
    "out, cache = affine_forward(norm_out, W, b)\n",
    "_, dout = cross_entropy_loss(out, y)\n",
    "dout, _, _ = affine_backward(dout, cache)\n",
    "dx, dgamma, dbeta = layernorm_backward(dout, norm_cache)\n",
    "grads = {\"x\":dx, \"gamma\":dgamma, \"beta\":dbeta}\n",
    "\n",
    "# These should all be less than 1e-8 or so.\n",
    "f = lambda a: cross_entropy_loss(affine_forward(layernorm_forward(x, gamma, beta, {})[0],\n",
    "                                          W, b)[0], y)[0]\n",
    "for _name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, params[_name], verbose=False)\n",
    "    print(\"%s max relative error: %e\" % (_name, rel_error(grad_numeric, grads[_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Norm\n",
    "\n",
    "When used with convolutional layers Layer Normalization does not perform as well as Batch Normalization. An observation made by the authors of [2] is that with fully connected layers all the hidden units in a layer tend to make similar contributions to the final prediction and re-centering and re-scaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of hidden units whose receptive fields lie near the boundary of the image are rarely turned on and thus have very different statistics from the rest of the hidden units within the same layer.\n",
    "\n",
    "Group normalization is an intermediary technique for normalizing inputs of shape $ (m, C, H, W) $. In contrast to Layer Normalization, where you would normalize over all the channels per-datapoint, in Group Normalization the channels per-datapoint are split into $ G $ groups, and the normalization is per-group per-datapoint.\n",
    "\n",
    "![GroupNorm](img/batchnorm_groupnorm.png \"GroupNorm\")\n",
    "\n",
    "The mean and the variance of a feature $ u^{(k)} $ are computed as follows:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "& \\mu_{G} = \\frac{1}{|G|} \\sum_{i \\in G} u_{i} \\\\\n",
    "& \\upsilon_{G}^{(k)} = \\frac{1}{|G|} \\sum_{i \\in G} (u_{i} - \\mu_{G})^{2} \\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "In this setting, within each group an assumption of equal contribution is being made. However, according to the authors of [3], this assumption is not as problematic.  \n",
    "One example they use to illustrate this is that many high-performance handcrafted features in traditional Computer Vision have terms that are explicitly grouped together. Take for example Histogram of Oriented Gradients [4]-- after computing histograms per spatially local block, each per-block histogram is normalized before being concatenated together to form the final feature vector.\n",
    "\n",
    "Obviously, if we set the group number $ G = 1 $ then Group Norm becomes Layer Norm. It assumes that <i>all</i> channels in a layer make \"similar contributions\".  \n",
    "Alternatively, if we set $ G = C $ (i.e. one channel per group), then Group Norm becomes Instance Norm. But Instance Norm can only rely on the spatial dimension for computing the mean and variance and it misses the opportunity of exploiting the channel dependance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
