{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD VECTORS\n",
    "\n",
    "The first and arguably the most important common denominator across all NLP tasks is how to represent words as input to any model. We also need to have some notion of similarity and difference between words.\n",
    "\n",
    "Word vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, machine translation, etc. It is important to build some intuition as to their strengths and weaknesses.\n",
    "\n",
    "We want to encode word tokens each into some vector that represents a point in some sort of \"word\" space. Ideally, each dimension would encode some meaning that we transfer using speech. With word vectors, we can quite easily compute similarity of words using different measures such as Jaccard coefficient, Cosine similarity or Euclidean distance.\n",
    "\n",
    "There are different methods to find word embeddings (word vectors). The most simple word vector is the <b>one-hot vector</b>. It represents every word as an $ \\mathbb{R}^{|V| x 1} $ vector with all 0s and one 1 at the index if that word in the dictionary:\n",
    "\n",
    "$$ V['cat'] = 2 \\implies w^{cat} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$\n",
    "\n",
    "However, this representation does not give us directly any notion of similarity. All the word vectors are pairwise orthogonal. Using cosine similarity as an example, we would get:\n",
    "\n",
    "![Motel Hotel](img/word2vec_motel_hotel.png \"Motel Hotel\")\n",
    "\n",
    "\n",
    "The idea is to try to reduce the size of this vector space from $ \\mathbb{R}^{|V|} $ to something smaller and thus find a subspace that enocdes the relationship between words. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space.\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words (or even synonyms) will be used in similar contexts and, by examining these contexts, we can try to develop embeddings for our words. This idea is based on the famous quotation by John Rupert Firth <i>\"You shall know a word by the company it keeps.\"</i>\n",
    "\n",
    "![Word Context](img/word2vec_context.png \"Word Context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "from word2vec import *\n",
    "from solver import *\n",
    "from layers import *\n",
    "from utils.gradient_check import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Methods\n",
    "\n",
    "Many \"old school\" approaches to constructing word vectors relied on word counts. We loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix $ X $.\n",
    "\n",
    "### Word-Document Matrix\n",
    "\n",
    "We could make the conjecture that words that are related will often appear in the same documents and we use this fact to build a word-document matrix $ X $ in the following manner: loop over billions of documents and for each time a word <i>i</i> appears in document <i>j</i>, we add one to the entry $ X_{ij} $.\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"  \n",
    "Document 2: \"all is well that ends well\"  \n",
    "\n",
    "|     *    | doc1 | doc2 |\n",
    "|----------|------|------|\n",
    "| all      |  1   |  1   |\n",
    "| that     |  1   |  1   |\n",
    "| glitters |  1   |  0   |\n",
    "| is       |  1   |  1   |\n",
    "| not      |  1   |  0   |\n",
    "| gold     |  1   |  0   |\n",
    "| well     |  0   |  2   |\n",
    "| ends     |  0   |  1   |\n",
    "\n",
    "Now, the rows of our matrix could be the vector embeddings of our words. However, the problem with this matrix is that it scales with the number of documents.\n",
    "\n",
    "### Window Co-occurrence Matrix\n",
    "\n",
    "Another approach we could try is to count the number of times each word appears inside a window of a particular size around the word of interest. Given a word in a document $ w_{i} $, we consider <i>the context window</i> of size $ n $ surrounding $ w_{i} $, i.e. the $ n $ preceeding and $ n $ subsequent words in that document ( $ w_{i-n}, ..., w_{i-1} $ and $ w_{i+1}, ..., w_{i+n} $ ).We build a co-occurrence matrix $ M $ in which $ M_{ij} $ is the number of times $ w_{j} $ occurs inside $ w_{i} $ 's window. To build the matrix we calculate this count for all the words and all the documents.\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"  \n",
    "Document 2: \"all is well that ends well\"  \n",
    "\n",
    "\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
    "\n",
    "<b>Note</b>: It is often the practice to add  `<START>` and `<END` tokens to represent the beginning and end of sentences, paragraphs or documents.\n",
    "\n",
    "Again, the rows of our matrix could be the vector embeddings of our words, however, the size of the vector space is still $ \\mathbb{R}^{|V|} $. To address this problem we can perform <i>dimensionality reduction</i> using <i>Singular Value Decomposition</i>. We decompose $ M = U \\Sigma V^{T} $ and select the top $ k $ principle components by truncating the matrix $ U $.\n",
    "\n",
    "In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform the decomposition. However, if you only want the top $ k $ vector components for a relatively small $ k $ then there are reasonably scalable techniques to compute those iteratively.\n",
    "\n",
    "<b>Note:</b> We could also perform SVD on the Word-Document matrix, however, the window-based assumption is much more reasonable. The Word-Document matrix could be regarded, in a sense, as a Window Co-occurrence matrix with variable window size equal to the length of the current document.\n",
    "\n",
    "\n",
    "## Iteration-based Methods\n",
    "\n",
    "Another approach is to try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context.\n",
    "\n",
    "The idea is to train a simple neural network with a single hidden layer to perform a cetain task, but then we're not actually going to use that neural network for the task we trained it on. Instead, the goal is to actually just learn the weights of the hidden layer. These weights are actually the \"word vectors\".\n",
    "\n",
    "A <b>language model</b> assigns probability to a sequence of tokens. A good language model will assign high probability to valid sentences and low probability to invalid sentences. Mathematically we can express this by:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=2}^{k} P(w_{i} | w_{i-1}, ..., w_{1}) $$\n",
    "\n",
    "for a sentence with $ k $ words.\n",
    "\n",
    "A <i>unary language model</i> assumes the word occurrences are completely independent:\n",
    "\n",
    "$$ P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=1}^{k} P(w_{i}) $$\n",
    "\n",
    "However, this model is not very accurate because we know that the next word is highly contingent upon the previous sequence of words. So we could try using an <i>n-gram language model</i> where we assume that each word occurence depends on the previous $ n-1 $ words.\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w_{1}, w_{2}, ..., w_{k}) = \\prod_{i=2}^{k} P(w_{i} | w_{i-1}, ..., w_{i-(n-1)}) $$\n",
    "\n",
    "The idea was further developed and two algorithms for learning word vectors were proposed in 2013 by Tomas Mikolov, Kai Chen, Greg Corrado and Jeff Dean in their paper:  \n",
    "<i>[1] \" Efficient Estimation of Word Representations in Vector Space\"</i>.\n",
    " * <b>Continuous bag-of-words (CBOW)</b>, which aims to predict a center word from the surrounding context.\n",
    " * <b>Skip-gram</b>, which aims to predict the distribution (probability) of context words from a center words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "#### The Fake Task\n",
    "The goal of the CBOW algorithm is: given context words to accurately learn to predict the center word.\n",
    "\n",
    "For given context words $ w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} $ the probability of the center word is:\n",
    "\n",
    "$$ P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) $$\n",
    "\n",
    "The exact ordering of the words in the context is ignored, only the number of occurances of each term is material, thus the name <i>bag-of-words</i>.\n",
    "\n",
    "Given specific context words $ w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} $, a question arrises how to calculate that probability $ P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) $. Assume we have <i>some</i> learned embeddings of the words (say $ u_{1}, u_{2}, ..., u_{|V|} $). For a given context around a center word $ w_{t} $, we take the average of the vectors of the context words:\n",
    "\n",
    "$$ \\upsilon_{t} = \\frac{u_{t-m} + ... + u_{t-1} + u_{t+1} + ... + u_{t+m}}{2m} $$\n",
    "\n",
    "Having obtained the vector $ \\upsilon_{t} $, we can use softmax classification to compute the probability for each word in the dictionary to be our center word.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} - \\space \\upsilon_{t} \\space - \\end{bmatrix}\n",
    "\\begin{bmatrix} | & | & \\cdots & | \\\\ \n",
    "                \\theta_{1} & \\theta_{2} & \\cdots & \\theta_{|V|} \\\\\n",
    "                | & | & \\cdots & |\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \\upsilon_{t} \\theta_{1} \\\\\n",
    "                \\upsilon_{t} \\theta_{2} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\upsilon_{t} \\theta_{|V|}\n",
    "\\end{bmatrix}\n",
    "\\underset{softmax}{\\Rightarrow}\n",
    "\\begin{bmatrix} \\frac{exp(\\upsilon_{t} \\theta_{1})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})} \\\\\n",
    "                \\frac{exp(\\upsilon_{t} \\theta_{2})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\frac{exp(\\upsilon_{t} \\theta_{|V|})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The weights of the softmax classifier are stored in a matrix $ W_{D x |V|} $.\n",
    "\n",
    "We now have:\n",
    "\n",
    "$$ P(w_{t} | w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}) = \\displaystyle\\frac{exp(\\upsilon_{t} \\theta_{t})}{\\displaystyle \\sum_{i=1}^{|V|}exp(\\upsilon_{t} \\theta_{i})} $$\n",
    "\n",
    "Let's say we have a vocabulary of $ |V| $ words, where each word is represented by a one-hot vector. If we store all the currently learned embeddings in a matrix $ U_{|V| x D} $ ($ D $ is the size of the embedding), then mapping a one-hot vector to its corresponding embedding can be seen as a vector-matrix multiplication. Multiplying a one-hot vector by a matrix effectively selects the matrix row whose index corresponds to the index of the 1 in the one-hot vector:\n",
    "\n",
    "$$ u_{t} =\n",
    "\\underbrace\n",
    "{\\begin{bmatrix} 0 & 0 & \\cdots & 0 & 1 & 0 & \\cdots & 0 \\end{bmatrix}}\n",
    "_{\\text{1 at position t}}\n",
    "\\begin{bmatrix} - \\space u_{1} \\space -  \\\\\n",
    "                - \\space u_{2} \\space -  \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{t} \\space -  \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space -\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Calculating the sum of the embeddings of some words can be computed by stacking the one-hot vectors of these words one after another into a $ 1 x 2m|V| $ vector and stacking the embedding matrix in the same manner into a $ 2m|V| x D $ matrix:\n",
    "\n",
    "$$ \\upsilon_{t} =\n",
    "\\begin{bmatrix}\n",
    "0 & \\cdots & 1 & \\cdots & 0 & \\cdots & \\cdots & 0 & \\cdots & 1 & \\cdots & 0\n",
    "\\end{bmatrix}_{1 x 2m|V|}\n",
    "\\begin{bmatrix} - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space - \\\\\n",
    "                - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space - \\\\\n",
    "                \\vdots \\\\\n",
    "                \\vdots \\\\\n",
    "                - \\space u_{1} \\space -  \\\\ \n",
    "                \\vdots \\\\\n",
    "                - \\space u_{|V|} \\space -\n",
    "\\end{bmatrix}_{2m|V| x D} $$\n",
    "\n",
    "The entire process can be modelled as training a two-layer neural network. There is no activation function for the neurons in the hidden layer. The weights of the first layer are the embeddings we are trying to learn and the weights of the second layer are the weights for a softmax classifier.\n",
    "\n",
    "![CBOW](img/word2vec_cbow.png \"CBOW\")\n",
    "\n",
    "In practice, instead of multiplying one-hot vectors with the embedding matrix, we can perform simple table lookups to obtain embeddings of the contextual words and, after that, sum the vectors to arrive at the desired result.\n",
    "\n",
    "#### The objective function\n",
    "\n",
    "For each position in a text $ t = 1, ..., T $ we predict the center word given the context words within a window of size $ m $. Thus, the likelihood of the text is given by the product of the probabilities of the center words:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{Likelihood =} L(\\eta)}_{\\substack{\\eta \\space \\text{is all variables} \\\\ \\text{ to be optimized}}} = \\displaystyle\\prod_{t=1}^{T} P(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \\eta) $$\n",
    "\n",
    "The objective function (the loss) is the average negative log of the likelihood:  \n",
    "\n",
    "\n",
    "$$ J(\\eta) = -\\frac{1}{T}logL(\\eta) = -\\frac{1}{T}\\sum_{t=1}^{T} logP(w_{t} |  w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}; \\eta) $$  \n",
    "\n",
    "\n",
    "$$ J(\\eta) = -\\frac{1}{T} \\sum_{t=1}^{T} log \\frac{exp(\\upsilon_{t}\\theta_{t})}{\\displaystyle \\sum_{i=1}^{|v|} exp(\\upsilon_{t}\\theta_{i})}$$  \n",
    "\n",
    "\n",
    "$$ J(\\eta) = \\frac{1}{T} \\sum_{t=1}^{T} (log \\sum_{i=1}^{|V|} exp(\\upsilon_{t}\\theta_{i}) -  \\upsilon_{t}\\theta_{t}) $$  \n",
    "\n",
    "The loss for each example is equal to the cross-entropy loss between the true distribution (the one-hot vector of the center word) and the predicted distribution.\n",
    "\n",
    "To perform one step in the optimization process, we need to find the gradients of the loss function with repsect to the parameters of the model $ \\frac{\\delta J}{\\delta U} $ and $ \\frac{\\delta J}{\\delta W} $. To calculate them we will use the chain rule:  \n",
    "\n",
    "\n",
    "$$ \\frac{\\delta J}{\\delta W} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} $$  \n",
    "\n",
    "\n",
    "$$ \\frac{\\delta J}{\\delta U} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} = \\frac{\\delta J}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} $$  \n",
    "\n",
    "\n",
    "$$ \\frac{\\delta J}{\\delta scores} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta s_{11}} & \\frac{\\delta J}{\\delta s_{12}} & \\cdots & \\frac{\\delta J}{\\delta s_{1|V|}} \\\\\n",
    "\\frac{\\delta J}{\\delta s_{21}} & \\frac{\\delta J}{\\delta s_{22}} & \\cdots & \\frac{\\delta J}{\\delta s_{2|V|}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\delta J}{\\delta s_{T1}} & \\frac{\\delta J}{\\delta s_{T2}} & \\cdots & \\frac{\\delta J}{\\delta s_{T|V|}}\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "\n",
    "$$ \\frac{\\delta J}{\\delta s_{ij}} = \\frac{\\delta J}{\\delta \\upsilon_{i}\\theta_{j}}\n",
    "= \\frac{1}{T} \\left( \\frac{exp(\\upsilon_{i}\\theta_{j})}{\\displaystyle \\sum_{k=1}^{|V|} exp(\\upsilon_{i}\\theta_{k})} - \\underbrace{1_{\\{y(i) = j\\}}}_{\\text{1 if the label of i is j}} \\right) $$  \n",
    "\n",
    "\n",
    "We will train the model by going through each word in the text and considering it as a center word. For every center word we consider a contextual window of a fixed size (for example <i>window_size</i>=2).\n",
    "\n",
    "![Word Context CBOW](img/word2vec_context_cbow.png \"Word Context CBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:\n",
      " [[5 0 3 3 9 3 5 2]\n",
      " [4 7 6 8 1 6 7 7]\n",
      " [8 1 5 9 9 4 3 0]\n",
      " [3 5 0 2 8 1 3 3]\n",
      " [3 7 0 1 9 0 4 7]]\n",
      "target:\n",
      " [[7]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize random data.\n",
    "np.random.seed(seed=0)\n",
    "N = 5\n",
    "T = 4\n",
    "V = 10\n",
    "D = 8\n",
    "\n",
    "X = np.random.randint(low=0, high=V, size=(N, 2*T + 1))\n",
    "context = np.concatenate((X[:, : T], X[:, T + 1 : ]), axis=1)\n",
    "target = X[:, T].reshape(N, 1)\n",
    "\n",
    "print(\"context:\\n\", context)\n",
    "print(\"target:\\n\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors:\n",
      " [[[-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.472 -0.065  0.099 -0.27  -0.807  0.207  0.273 -0.235]]\n",
      "\n",
      " [[-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [-0.51  -0.067 -0.283  0.122 -0.162 -0.373 -0.009  0.135]\n",
      "  [ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [-0.51  -0.067 -0.283  0.122 -0.162 -0.373 -0.009  0.135]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [-0.332 -0.449 -0.54   0.617 -0.161 -0.139 -0.396  0.246]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [ 0.472 -0.065  0.099 -0.27  -0.807  0.207  0.273 -0.235]\n",
      "  [ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [-0.033  0.13   0.046  0.46   0.241  0.038  0.14   0.106]\n",
      "  [ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]\n",
      "  [ 0.558  0.127  0.31   0.709  0.591 -0.309  0.3   -0.048]\n",
      "  [-0.281 -0.626 -0.11   0.049  0.389  0.38  -0.122 -0.096]\n",
      "  [ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the forward pass.\n",
    "np.random.seed(seed=0)\n",
    "CBOW = word2vec(vocab_size=V, embed_size=D,\n",
    "                model_type=\"cbow\", dtype=np.float64)\n",
    "\n",
    "word_vectors, _ = word_embedding_forward(context, CBOW.params[\"U\"])\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"word vectors:\\n\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W max relative error: 5.409319e-08\n",
      "U max relative error: 2.545553e-07\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass.\n",
    "loss, grads = CBOW.loss(X)\n",
    "\n",
    "f = lambda W: CBOW.loss(X)[0]\n",
    "for name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, CBOW.params[name], verbose=False)\n",
    "    print('%s max relative error: %e' % (name, rel_error(grad_numeric, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "\n",
    "#### The Fake Task\n",
    "\n",
    "The goal of the Skip-Gram algorithm is: given a \"ceneter\" word $ w_{t} $ (the input word) to accurately learn the probability distribution of the contextual window surrounding $ w_{t} $. The output distribution is going to relate to how likely is to find each vocabulary word nearby our input word.\n",
    "\n",
    "For a given center word $ w_{t} $ the probability of the context words is:\n",
    "\n",
    "$$ P(w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} | w_{t}) $$\n",
    "\n",
    "A key difference with <i>CBOW</i> is that <b>we make a Naive Bayes assumtion for conditional independence</b>. In other words, given the center word, all output words are independent.\n",
    "\n",
    "$$ P(w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m} | w_{t}) \\space = \\displaystyle \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j}|w_{t}) $$\n",
    "\n",
    "Given a specific word $ w_{k} $ we want to calculate the probability $ P(w_{k}|w_{t}) $, which is the probability that word $ w_{k} $ falls within the contextual window of $ w_{t} $. Again, assuming we have <i>some</i> learned embeddings of the words (say $ u_{1}, u_{2}, ..., u_{|V|} $), we can use softmax classification to compute that probability.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} - \\space u_{t} \\space - \\end{bmatrix}\n",
    "\\begin{bmatrix} | & | & \\cdots & | \\\\\n",
    "                \\theta_{1} & \\theta_{2} & \\cdots & \\theta_{|V|} \\\\\n",
    "                | & | & \\cdots & |\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} u_{t} \\theta_{1} \\\\\n",
    "                u_{t} \\theta_{2} \\\\\n",
    "                \\vdots \\\\\n",
    "                u_{t} \\theta_{|V|}\n",
    "\\end{bmatrix}\n",
    "\\underset{softmax}{\\Rightarrow}\n",
    "\\begin{bmatrix} \\frac{exp(u_{t} \\theta_{1})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})} \\\\\n",
    "                \\frac{exp(u_{t} \\theta_{2})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\frac{exp(u_{t} \\theta_{|V|})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We now have:\n",
    "\n",
    "$$ P(w_{k}|w_{t}) = \\displaystyle \\frac{exp(u_{t} \\theta_{k})}{\\displaystyle \\sum_{i=1}^{|V|}exp(u_{t} \\theta_{i})} $$\n",
    "\n",
    "Again, If we store all the currently learned embeddings in a matrix $ U_{|V| x D} $, the entire process can be modelled as training a two-layer neural network, and again, instead of multiplying a one-hot vector with the embedding matrix, we perform a simple table lookup to obtain the desired result.\n",
    "\n",
    "![Skip-Gram](img/word2vec_skip-gram.png \"Skip-Gram\")\n",
    "\n",
    "\n",
    "#### The objective function\n",
    "\n",
    "For each position in the text $ t = 1, ..., T $ we predict the context words within a window of size $ m $. Thus, the likelihood of the text is given by the product of the probabilities of the context words for every center word:\n",
    "\n",
    "$$ \\underbrace{\\text{Likelihood =} L(\\eta)}_{\\substack{\\eta \\space \\text{is all variables} \\\\ \\text{ to be optimized}}} = \\displaystyle \\prod_{t=1}^{T} \\displaystyle \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j}|w_{t}; \\eta) $$\n",
    "\n",
    "The objective function (the loss) is the average negative log of the likelihood:\n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T}logL(\\eta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} logP(w_{t+j}|w_{t}; \\eta) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq k \\leq m \\\\ k \\neq 0}} log \\frac{exp(u_{t}\\theta_{t + k})}{\\sum_{l=1}^{|V|} exp(u_{t}\\theta_{l})} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = \\frac{1}{T} \\sum_{t=1}^{T} \\left( 2m \\space log \\sum_{l=1}^{|V|} exp(u_{t}\\theta_{l}) \\space \\space - \\sum_{\\substack{-m \\leq k \\leq m \\\\ k \\neq 0}} u_{t}\\theta_{t + k} \\right) $$  \n",
    "\n",
    "The loss for each example is equal to the sum over the contextual words of the cross-entropy loss between the true distribution of a contextual word (the one-hot vector of the contextual word) and the predicted distribution.\n",
    "\n",
    "To calculate the gradients of the loss function with repsect to the parameters of the model $ \\displaystyle \\frac{\\delta J}{\\delta U} $ and $ \\displaystyle \\frac{\\delta J}{\\delta W} $ we will use the chain rule:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta W} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta U} = \\frac{\\delta J}{\\delta scores} \\frac{\\delta scores}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} = \\frac{\\delta J}{\\delta W} \\frac{\\delta W}{\\delta U} \\frac{\\delta U}{\\delta X} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta scores} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\delta J}{\\delta s_{11}} & \\frac{\\delta J}{\\delta s_{12}} & \\cdots & \\frac{\\delta J}{\\delta s_{1|V|}} \\\\\n",
    "\\frac{\\delta J}{\\delta s_{21}} & \\frac{\\delta J}{\\delta s_{22}} & \\cdots & \\frac{\\delta J}{\\delta s_{2|V|}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\delta J}{\\delta s_{T1}} & \\frac{\\delta J}{\\delta s_{T2}} & \\cdots & \\frac{\\delta J}{\\delta s_{T|V|}}\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\frac{\\delta J}{\\delta s_{ij}} = \\frac{\\delta J}{\\delta u_{i}\\theta_{j}}\n",
    "= -\\frac{2m}{T} \\left( \\frac{exp(u_{i}\\theta_{j})}{\\displaystyle \\sum_{l=1}^{|V|} exp(u_{i}\\theta_{l})} - \\underbrace{1_{\\{y(i) = j\\}}}_{\\text{1 if the label of i is j}} \\right) $$  \n",
    "\n",
    "\n",
    "\n",
    "#### Training the model\n",
    "\n",
    "We will train the model by going through each word in the text and considering it as a center word. For every center word we consider a contextual window of a fixed size (for example <i>window_size=2</i>).\n",
    "\n",
    "![Word context Skip-Gram](img/word2vec_context_skip-gram.png \"Word Context Skip-Gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors:\n",
      " [[[ 0.021  0.096 -0.201 -0.115 -0.213 -0.114 -0.257 -0.546]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]]\n",
      "\n",
      " [[ 0.056 -0.127 -0.516  0.146 -0.287  0.016  0.231  0.041]]\n",
      "\n",
      " [[ 0.718 -0.46   0.014 -0.059  0.485  0.465  0.049  0.12 ]]\n",
      "\n",
      " [[ 0.36  -0.39   0.127 -0.217 -0.275 -0.183 -0.099  0.018]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the forward pass.\n",
    "np.random.seed(seed=0)\n",
    "SkipGram = word2vec(vocab_size=V, embed_size=D,\n",
    "                    model_type=\"skipgram\", dtype=np.float64)\n",
    "\n",
    "word_vectors, _ = word_embedding_forward(target, SkipGram.params[\"U\"])\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"word vectors:\\n\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W max relative error: 2.184301e-08\n",
      "U max relative error: 3.042708e-09\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass.\n",
    "loss, grads = SkipGram.loss(X)\n",
    "\n",
    "f = lambda W: SkipGram.loss(X)[0]\n",
    "for name in grads:\n",
    "    grad_numeric = eval_numerical_gradient(f, SkipGram.params[name], verbose=False)\n",
    "    print('%s max relative error: %e' % (name, rel_error(grad_numeric, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "\n",
    "<b>NOTE 1:</b> Obviously, with no non-linearity in the hidden layer the two matrix multiplications can be collapsed into a single matrix, and the entire neural network transforms into a linear classifier. However, as stated earlier, the goal is not to train a classifier, but to learn the weights of the hidden layer.\n",
    "\n",
    "<b>NOTE 2:</b> It is common to see the weights of the softmax classifier also reffered to as word embeddings. For CBOW $ u_{t} $ is the embedding of $ w_{t} $ when it is a center word, and $ \\theta_{t} $ is the embedding of $ w_{t} $ when it is a context word. And vice-versa for Skip-Gram. It is also common to use both vectors to arrive at the final embedding for the word $ w_{t} $ (via concatenation or averaging).\n",
    "\n",
    "<b>NOTE 3:</b> The neural network does not know anything about the offset of the output word relative to the input word. More distant words are usually less related to the current word than those close to it. To address this problem, the authors of the paper propose to give less weight to the distant words by sampling less from those words in our training examples. That is, the parameter $ m $ denotes the <i>maximal window size</i>. For each center word we select uniformly a number $ R $ in the range $ [1, m] $ and use $ R $ as the window size for this center word. This technique is referred to as <i>dynamic window size</i>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It deos not learn a different set of probabilities for the word before the input versus the word after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors\n",
    "\n",
    "We will train word vectors using the Skip-Gram model on Wikipedia articles.\n",
    "As stated earlier, we are interested in the model's parameters, not in its classification capabilities. However, if two different words appear in similar \"contexts\", then our model needs to output very similar results for these two words. And one way for the network to output similar results is if the word vectors for the context words are similar. Thus, our network is motivated to learn similar word vectors for similar words.  \n",
    "We will learn word vector representations for the most common words in the text and will replace rare words with `UNK` token. Vocabulary size greatly impacts the computation time needed for the model to learn the vector representations. Thus, we will constrain ourselves with a relatively small vocabulary.  \n",
    "The model uses L2 regularization of the weight matrices.  \n",
    "Parameter update is performed using Adam update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data.\n",
    "# filename = \"datasets/text/HarryPotter_book1.txt\"\n",
    "# with open(filename, \"r\") as file:\n",
    "#     text = file.read()\n",
    "#     text = text.split()\n",
    "\n",
    "# print(\"Length of the text: %d words\" % len(text))\n",
    "# print(\"Example text:\\n\\n\", text[:57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text...\n",
      "Done!\n",
      "Number of words in the dataset:  1000000\n",
      "\n",
      "Example words:  ['as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upon', 'voluntary']\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "text8_dir = \"datasets/text8/\"\n",
    "with open(text8_dir + \"text8\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    text = text.split()\n",
    "\n",
    "text = text[:1000000]\n",
    "\n",
    "print(\"Number of words in the dataset: \", len(text))\n",
    "print(\"\\nExample words: \", text[150:165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      " {'UNK': 144423, 'the': 62827, 'of': 36789, 'and': 25238, 'one': 24679, 'in': 22502, 'a': 18620, 'to': 18504, 'zero': 14349, 'nine': 14056, 'is': 11094, 'two': 10968, 'as': 7737, 'eight': 7708, 'three': 7049, 'was': 6892, 'by': 6796, 'five': 6647, 's': 6606, 'that': 6541, 'for': 6447, 'four': 6338, 'six': 6239, 'seven': 5914, 'with': 5672}\n",
      "Vocab:\n",
      " {'UNK': 0, 'the': 1, 'of': 2, 'and': 3, 'one': 4, 'in': 5, 'a': 6, 'to': 7, 'zero': 8, 'nine': 9, 'is': 10, 'two': 11, 'as': 12, 'eight': 13, 'three': 14, 'was': 15, 'by': 16, 'five': 17, 's': 18, 'that': 19, 'for': 20, 'four': 21, 'six': 22, 'seven': 23, 'with': 24}\n",
      "\n",
      "Vocabulary size: 5000\n",
      "Example text data:\n",
      " ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been']\n",
      "After preprocessing:\n",
      " [632, 3668, 12, 6, 173, 2, 4288, 47, 63, 150, 124, 870, 666, 0, 161, 1, 0, 2, 1, 111, 949, 3, 1, 0, 0, 2, 1, 138, 949, 4000, 1, 173, 10, 179, 63, 5, 6, 0, 212, 7, 1230, 105, 450, 19, 63, 2302, 365, 7, 3106, 1, 1049, 2, 367, 30, 41, 38, 52]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to create word-to-int and int-to-word mappings.\n",
    "# We will index the most common words and replace rare words with \"UNK\" token.\n",
    "# collections.Counter(list).most_common(N) returns a tuple of the N most common instances\n",
    "# contained in the list along with their count number\n",
    "vocab_size = 5000\n",
    "count = [(\"UNK\", 0)]\n",
    "count.extend(collections.Counter(text).most_common(vocab_size - 1))\n",
    "count = dict(count)\n",
    "count[\"UNK\"] = len(text) - sum(count.values())\n",
    "print(\"Most common words:\\n\", dict(itertools.islice(count.items(), 25)))\n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(count)}\n",
    "idx_to_word = {i: w for i, w in enumerate(count)}\n",
    "print(\"Vocab:\\n\", dict(itertools.islice(word_to_idx.items(), 25)))   # slicing a dictionary with itertools.islice\n",
    "\n",
    "data = [word_to_idx.get(w, word_to_idx[\"UNK\"]) for w in text]\n",
    "\n",
    "print(\"\\nVocabulary size: %d\" % vocab_size)\n",
    "print(\"Example text data:\\n\", text[:57])\n",
    "print(\"After preprocessing:\\n\", data[:57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(object):\n",
    "    def __init__(self, data, window_size):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - data: List of integers representing the preprocessed data.\n",
    "          Every integer corresponds to a character.\n",
    "        - window_size: Integer giving the contextual window size.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.span = 2 * window_size + 1 # window - target - window\n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate the next batch of examples from the data.\n",
    "\n",
    "        Returns:\n",
    "        - batch: A numpy array of integers of shape (batch_size, seq_length) giving\n",
    "          a batch of training examples.\n",
    "        \"\"\"\n",
    "        span = self.span\n",
    "        batch = np.ndarray((batch_size, span), dtype=np.int)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            cursor = np.random.randint(len(self.data)-span)\n",
    "            batch[idx] = self.data[cursor : cursor + span]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def num_train(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - num_train: Integer, giving the number of training examples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset.\n",
    "np.random.seed(seed=None)\n",
    "window_size = 2\n",
    "dataset = text_dataset(data, window_size)\n",
    "\n",
    "# Initialize the model\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "SkipGram = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                    model_type=\"skipgram\",\n",
    "                    word_to_idx=word_to_idx,\n",
    "                    dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations per epoch: 7812\n",
      "(Iteration 0 / 39060); Epoch(0 / 5); loss: 8.51738\n",
      "Sample:\n",
      " Nearest to friend: arian caesar else club jerome ordinary atomic wildlife were societies\n",
      "(Iteration 5000 / 39060); Epoch(0 / 5); loss: 5.42829\n",
      "Sample:\n",
      " Nearest to friend: father wife fellow son grandson friends happiness lover marriage brother\n",
      "(Iteration 10000 / 39060); Epoch(1 / 5); loss: 5.50250\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream julia childhood fellow mother wilson vision followers\n",
      "(Iteration 15000 / 39060); Epoch(1 / 5); loss: 5.57384\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream milne lady cousin portrait mother biography julia\n",
      "(Iteration 20000 / 39060); Epoch(2 / 5); loss: 5.33698\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream cousin friends mother milne miss critic horse\n",
      "(Iteration 25000 / 39060); Epoch(3 / 5); loss: 5.43614\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream cousin friends milne mother daughter horse fellow\n",
      "(Iteration 30000 / 39060); Epoch(3 / 5); loss: 5.23789\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream cousin friends milne fellow horse mother son\n",
      "(Iteration 35000 / 39060); Epoch(4 / 5); loss: 5.27505\n",
      "Sample:\n",
      " Nearest to friend: lover wife dream cousin milne friends son critic mother horse\n",
      "training took 254.120 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "word2vec_solver = UnsupervisedSolver(SkipGram, dataset,\n",
    "                                     update_rule=\"adam\",\n",
    "                                     optim_config={\"learning_rate\": 1e-2},\n",
    "                                     lr_decay=0.5,\n",
    "                                     batch_size=batch_size,\n",
    "                                     num_epochs=5,\n",
    "                                     print_every=5000,\n",
    "                                     verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "word2vec_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAEWCAYAAACgzMuWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzdd3gU5fYH8O9JIyR0CDVA6EoLJfQuHRQbKnrt+kPs7eqNXgUsIIoV8cpFvaiI2Ghq6EjvvXcIEGpCSSX9/f2xO5vZ3dnd2SSbDfL9PA8Pyczs7JtNsjlz5rznFaUUiIiIiIjIvAB/D4CIiIiI6FrDIJqIiIiIyEsMoomIiIiIvMQgmoiIiIjISwyiiYiIiIi8xCCaiIiIiMhLDKKJiP5mRORbEXnXzf40EWlYkmMiIvq7YRBNROQjIhIvIv38PQ5HSqlySqlj7o4Rkd4iklBSYyIiutYwiCYiomInIkH+HgMRkS8xiCYiKmEiUkZEPhWRM9Z/n4pIGeu+aiLyp4hcEZFLIrJaRAKs+/4lIqdFJFVEDopIXzdPU1lE4qzHbhSRRrrnVyLS2PrxEBHZZz3utIj8U0TCASwAUNta+pEmIrU9jLu3iCRYx3gOwDQR2SMit+ieN1hEkkSkTfG/qkREJYtBNBFRyfs3gM4A2gCIBtARwBvWfS8DSAAQAaAGgNcBKBFpBuAZAB2UUuUBDAQQ7+Y57gXwFoDKAI4AGOfiuG8APGE9Z0sAfyml0gEMBnDGWvpRTil1xsO4AaAmgCoA6gMYCeB7APfr9g8BcFYptcPNuImIrgkMoomISt4/ALytlLqglEqEJdh9wLovB0AtAPWVUjlKqdVKKQUgD0AZAM1FJFgpFa+UOurmOWYrpTYppXIBzIAl8DWSYz1nBaXUZaXUtkKOGwDyAYxRSmUppa4C+AHAEBGpYN3/AIDpbs5PRHTNYBBNRFTyagM4ofv8hHUbAEyEJXO8WESOiUgsACiljgB4AcBYABdE5CcRqQ3Xzuk+zgBQzsVxd8KSIT4hIitFpEshxw0AiUqpTO0Ta/Z6LYA7RaQSLNntGW7OT0R0zWAQTURU8s7AUvKgqWfdBqVUqlLqZaVUQwC3AHhJq31WSv2olOpufawC8H5RB6KU2qyUuhVAdQBzAfyi7fJm3G4e8x0sJR13AVivlDpd1DETEZUGDKKJiHwrWERCdf+CAMwE8IaIRIhINQCjYSl9gIjcLCKNRUQApMBSxpEnIs1E5CbrRL5MAFet+wpNREJE5B8iUlEplaN7PgA4D6CqiFTUPcTluN2YC6AdgOdhqZEmIvpbYBBNRORb82EJeLV/YwG8C2ALgF0AdgPYZt0GAE0ALAWQBmA9gP8opVbAUg89AUASLKUa1WGZdFhUDwCIF5EUAKNgnQiolDoAS9B8zNoppLaHcRuy1kbPAtAAwOxiGC8RUakglvkqREREviEiowE0VUrd7/FgIqJrBJvhExGRz4hIFQCPwb6LBxHRNY/lHERE5BMi8n8ATgFYoJRa5e/xEBEVJ5ZzEBERERF5iZloIiIiIiIvXXM10dWqVVNRUVH+HgYRERER/c1t3bo1SSkVYbTvmguio6KisGXLFn8Pg4iIiIj+5kTkhKt9LOcgIiIiIvISg2giIiIiIi8xiCYiIiIi8hKDaCIiIiIiLzGIJiIiIiLyEoNoIiIiIiIv+TSIFpEXRWSviOwRkZkiEuqwv7eIJIvIDuu/0b4cDxERERFRcfBZEC0idQA8ByBGKdUSQCCAEQaHrlZKtbH+e9tX4ymK5Ks5+HjJIew5nezvoRARERFRKeDrco4gAGVFJAhAGIAzPn4+n8jIzsWkZYexm0E0EREREcGHQbRS6jSADwGcBHAWQLJSarHBoV1EZKeILBCRFkbnEpGRIrJFRLYkJib6asguBQdaXqacvPwSf24iIiIiKn18Wc5RGcCtABoAqA0gXETudzhsG4D6SqloAJ8DmGt0LqXUVKVUjFIqJiLCcPlynwoJsrxM2bkMoomIiIjIt+Uc/QAcV0olKqVyAMwG0FV/gFIqRSmVZv14PoBgEanmwzEVSog1E53NTDQRERERwbdB9EkAnUUkTEQEQF8A+/UHiEhN6z6ISEfreC76cEyFYguimYkmIiIiIlgm/vmEUmqjiPwGS8lGLoDtAKaKyCjr/ikAhgN4UkRyAVwFMEIppXw1psIKCBAEBQhroomIiIgIgA+DaABQSo0BMMZh8xTd/skAJvtyDMUlODCAmWgiIiIiAsAVC00LChDk5pe6JDkRERER+QGDaLMEKH2FJkRERETkDwyiTQqwzH8kIiIiImIQbZYIkM9UNBERERGBQbRpASIs5yAiIiIiAAyiTRMwE01EREREFgyiTRIBGEITEREREcAg2jRhOQcRERERWTGINkkAlMLFFImIiIjIDxhEmyTsE01EREREVgyiTQoQ4cRCIiIiIgLAINo0AScWEhEREZEFg2iTOLGQiIiIiDQMok2y1EQziiYiIiIiBtGmsU80EREREWkYRJtkWfabYTQRERERMYg2zbLst79HQURERESlgU+DaBF5UUT2isgeEZkpIqEO+0VEJonIERHZJSLtfDmeoggQYTkHEREREQHwYRAtInUAPAcgRinVEkAggBEOhw0G0MT6bySAL301niITsE80EREREQHwfTlHEICyIhIEIAzAGYf9twL4XllsAFBJRGr5eEyFIgBnFhIRERERAB8G0Uqp0wA+BHASwFkAyUqpxQ6H1QFwSvd5gnWbHREZKSJbRGRLYmKir4bslqWcg1E0EREREfm2nKMyLJnmBgBqAwgXkfsdDzN4qFOkqpSaqpSKUUrFREREFP9gTRAB8vP98tREREREVMr4spyjH4DjSqlEpVQOgNkAujockwCgru7zSDiXfJQKzEQTERERkcaXQfRJAJ1FJExEBEBfAPsdjvkdwIPWLh2dYSn5OOvDMRUJW9wREREREWCZ+OcTSqmNIvIbgG0AcgFsBzBVREZZ908BMB/AEABHAGQAeMRX4ykqEQGbcxARERER4MMgGgCUUmMAjHHYPEW3XwF42pdjKC4BbM9BRERERFZcsdAkEZZzEBEREZEFg2iTBALFeg4iIiIiAoNo0wKYiSYiIiIiKwbRZomwIpqIiIiIADCINi1AwHIOIiIiIgLAINo0AdjijoiIiIgAMIg2TbhiIRERERFZMYg2yVLO4e9REBEREVFpwCDaJIEgn1E0EREREYFBtGnCTDQRERERWTGINolBNBERERFpGESbJODEQiIiIiKyYBBtUkAAM9FEREREZMEg2iROLCQiIiIiDYNok0TAYg4iIiIiAsAg2jQRQT6jaCIiIiICg2jTBGBRNBEREREB8GEQLSLNRGSH7l+KiLzgcExvEUnWHTPaV+MpqgCWcxARERGRVZCvTqyUOgigDQCISCCA0wDmGBy6Wil1s6/GUVws5RwMo4mIiIio5Mo5+gI4qpQ6UULPV+wErOYgIiIiIouSCqJHAJjpYl8XEdkpIgtEpIXRASIyUkS2iMiWxMRE343SDU4sJCIiIiKNz4NoEQkBMAzArwa7twGor5SKBvA5gLlG51BKTVVKxSilYiIiInw3WDcsy34ziiYiIiKikslEDwawTSl13nGHUipFKZVm/Xg+gGARqVYCY/JagPh7BERERERUWpREEH0vXJRyiEhNERHrxx2t47lYAmPyGlcsJCIiIiKNz7pzAICIhAHoD+AJ3bZRAKCUmgJgOIAnRSQXwFUAI1QprZmwlHP4exREREREVBr4NIhWSmUAqOqwbYru48kAJvtyDMUlQIR9oomIiIgIAFcsNE/Acg4iIiIiAsAg2rQA4ZKFRERERGTBINokATPRRERERGTBINokJqKJiIiISMMg2qQAEXbnICIiIiIADKJNYzkHEREREWkYRJskzEQTERERkRWDaJMsi60wiiYiIiIiBtGmCTixkIiIiIgsGESbxImFRERERKRhEG2ScMVCIiIiIrJiEG0S+0QTERERkYZBtEnszkFEREREGgbRJmXl5CMpLcvfwyAiIiKiUoBBtEmztiUAAJIzcvw8EiIiIiLyNwbRXsrIyfX3EIiIiIjIzxhEe4l10URERETksyBaRJqJyA7dvxQRecHhGBGRSSJyRER2iUg7X42nuDCGJiIiIqIgX51YKXUQQBsAEJFAAKcBzHE4bDCAJtZ/nQB8af2/1OLS30RERERUUuUcfQEcVUqdcNh+K4DvlcUGAJVEpFYJjalQGEMTERERUUkF0SMAzDTYXgfAKd3nCdZtpRaDaCIiIiLyeRAtIiEAhgH41Wi3wTanMFVERorIFhHZkpiYWNxD9Mqm+Et+fX4iIiIi8r+SyEQPBrBNKXXeYF8CgLq6zyMBnHE8SCk1VSkVo5SKiYiI8NEwzfl23XG/Pj8RERER+V9JBNH3wriUAwB+B/CgtUtHZwDJSqmzJTCmQttzOsXfQyAiIiIiP/NZdw4AEJEwAP0BPKHbNgoAlFJTAMwHMATAEQAZAB7x5XiKgxgVoBARERHRdcWnQbRSKgNAVYdtU3QfKwBP+3IMxY0xNBERERFxxUIvCVPRRERERNc9BtFeYghNRERERAyiiYiIiIi8xCDapEe6RQEwaGJNRERERNcdBtEmlStjmYOpuGQhERER0XWPQbRJOXmW4DmfMTQRERHRdc9UEC0i4SISYP24qYgME5Fg3w6tdPlp80l/D4GIiIiISgmzmehVAEJFpA6AZbAsivKtrwZVGmXl5Pt7CERERERUSpgNosW6cModAD5XSt0OoLnvhlX6BAawuR0RERERWZgOokWkC4B/AIizbvPpaoelDUNoIiIiItKYDaJfAPAagDlKqb0i0hDAct8Ni4iIiIio9DKVTVZKrQSwEgCsEwyTlFLP+XJgpU0AyzmIiIiIyMpsd44fRaSCiIQD2AfgoIi84tuhlS6MoYmIiIhIY7aco7lSKgXAbQDmA6gH4AGfjaoUEmEUTUREREQWZoPoYGtf6NsAzFNK5eA6WwGbITQRERERacwG0f8FEA8gHMAqEakPIMVXgyqNmIkmIiIiIo3ZiYWTAEzSbTohIn18MyQiIiIiotLN7MTCiiLysYhssf77CJas9HUjKS3L30MgIiIiolLCbDnH/wCkArjb+i8FwDRPDxKRSiLym4gcEJH91gVb9Pt7i0iyiOyw/hvt7RdARERERFTSzK462Egpdafu87dEZIeJx30GYKFSariIhAAIMzhmtVLqZpPj8JvAAEFe/nU1l5KIiIiIXDCbib4qIt21T0SkG4Cr7h4gIhUA9ATwDQAopbKVUlcKO1B/a1K9nL+HQERERESlhNkgehSAL0QkXkTiAUwG8ISHxzQEkAhgmohsF5GvrYu1OOoiIjtFZIGItDA6kYiM1OqxExMTTQ65eN0SXdv2cW5evl/GQERERESlg6kgWim1UykVDaA1gNZKqbYAbvLwsCAA7QB8aT0+HUCswzHbANS3nvtzAHNdPP9UpVSMUiomIiLCzJB9avqGE/4eAhERERH5kdlMNABAKZViXbkQAF7ycHgCgASl1Ebr57/BElQ7ni/N+vF8WBZ1qebNmEqKvk108tUc/w2EiIiIiPzOqyDagdvVR5RS5wCcEpFm1k19AeyzO4FITbGuYiIiHa3juViEMflM3coFcyIX7jnnx5EQERERkb+Z7c5hxEyrimcBzLB25jgG4BERGQUASqkpAIYDeFJEcmGZqDhCKVUqW2DcWKu87eMD51L9OBIiIiIi8je3QbSIpMI4WBYAZT2dXCm1A0CMw+Ypuv2TYZmkeA3gst9EREREZOE2iFZKlXe3/3pSs2Kov4dARERERKVEUWqiryvhIYH+HgIRERERlRIMok0SYTkHEREREVkwiC6kOdsTkJaV6+9hEBEREZEfMIgupBd/3ol/z9nt72EQERERkR8wiC6CLfGX/T0EIiIiIvIDBtFFkJWb7+8hEBEREZEfMIgugqS0LH8PgYiIiIj8gEF0Ef2y+RRe/mUncvKYlSYiIiK6XjCILqJXZ+3CrG0J2HT8kuH+U5cysPUEa6eJiIiI/k7crlhI5imjxdEB9PhgOQAgfsLQEhwNEREREfkSM9HFJDUzx99DICIiIqISwiC6mDw5Yxvy8y3p6FOXMvDF8iNQuvT0vB2n7T4nIiIiomsXyzl84KH/bcKxpHRUKxdi2/b8TzsQFhKE/s1r+HFkRERERFQcmIkuRp8tO4xTlzKQnm1ZDvxfs+xXNLySke30mLnbT2Pu9tMlMj4iIiIiKh4MoovRZ8sOo8cHy3E+xXX/6Px8hRkbTyDbulDLCz/vwAs/7/B47qX7zuNCaqapcSRczsC+MynmBk1EREREXmMQXYJEBHN3nMa/5+zB5OVHDI/Jys3DiYvpdtty8/Lx+PdbcO/UDaaep/v7yzFk0uoij5eIiIiIjPk0iBaRSiLym4gcEJH9ItLFYb+IyCQROSIiu0SknS/H429KKXxhDZ4vpzuXdgDAa7N2o9fEFUjRdfvIs05IPHkpw/eDJCIiIiKPfJ2J/gzAQqXUDQCiAex32D8YQBPrv5EAvvTxePzuaKIly6xg3Klj9ZEkAEBmdp5tm9bUQ0Rcnvd4UjqmrDxaTKMkIiIiInd81p1DRCoA6AngYQBQSmUDcEy/3grge2Xp/bbBmrmupZQ666tx+dMPG0/aPnbsdpeZk4d1R5Pstm+Jv4TQ4EA0iigHAAhwHUNjxNT1OJ+Shfs61SvSGNOycpGamYNaFcsW6TxEREREf2e+bHHXEEAigGkiEg1gK4DnlVL6gt86AE7pPk+wbrMLokVkJCyZatSrV7QgsSgaRYTbMsmFsfPUFdvHMzaexGVdt453/tyHGbogGwIMn7IeANDvxhrWTQVR9Nztp9GpYRVbsJthzVyr/IJTfLjoIF4e0NSWwf54ySEkpmbivTtauxzjLZ+vwfGkdK6wSEREROSGL8s5ggC0A/ClUqotgHQAsQ7HGOVWneoclFJTlVIxSqmYiIiI4h+pSYHuUsGFMH/3OdvHJy66rndeuv88AOBqTh52JVxBZk4eXvh5h91EQ6ORTV5+BOdTsnDSeu5Jyw5j5qZTyM9X2JVwxeARlrIQIiIiInLPl0F0AoAEpdRG6+e/wRJUOx5TV/d5JIAzPhxTkdzcurbPzr3GWgvtybDJa5Fvrfk4n5KFtKxcJKYWtNRzrLV+YvoW9Jy43G5bw9fnY9jktdh64rLL59l28jKS0ly36iMiIiK6nvksiFZKnQNwSkSaWTf1BbDP4bDfATxo7dLRGUByaa6Hfqp3oxJ7LjHMLVu8+tsuAJaAue9HK9Bh3FJbycbZZPte0jsTkgEAP206CUda8D35r8N48oetdvvu+M863Dp5rcsxZGTn4ps1x21LnRMRERFdT3y97PezAGaISAiAYwAeEZFRAKCUmgJgPoAhAI4AyADwiI/HUyQBbrpjlKQ/dxVcZ2gLuyRftbTEc9WhI3b2bqdtIUGWr+fDxYcMH3P6ylWXY5iw4AC+X38CkZXLYmCLmk77T17MQEhQAGpWDLVt23cmBQ2qhaNsSKDL85qxcM85RFYui5Z1KhbpPEZy8/JxLiUTkZXDiv3cZN6Ji+moXj60yD8rREREvuLTFndKqR3WWubWSqnblFKXlVJTrAE0lMXTSqlGSqlWSqktvhxPUZVkDD3KITNsxLHDBwDM22G+GsYx2x1vUA/9/fp4pGflOm3XgvarulZ8Jy6m20pEek5cjs7vLcNPm07i4LlUpGbmYMik1Xjh5+0OX4PC0cQ002MGLK/NzZ+vsT2/MnohCmn8/APo/v5yt6tDHj6fivx8hTHz9uCYl2Mnc3pNXIGR00v12wEREV3nuGKhF9z1aS5u7uqVNVm5+R6PccexfnrAp6ucjhk9by/Gzbe0976anYfPlx1Gbl4+UjMtgXVuvkJUbBw+WHgAvSauwJ1frrN7fOzs3Rj46Spk5ljG6vh1/bjpJPp+tBKbjl+y2970jQV464+9OJqYZlsi3dGF1EzcOHohpqw85sVXDcTO2oWXf9mJhMvOkzlXH04EAFzJyHHaBwCrDiWi/yerMGHhAXy3/gSemO75Ysefnv9pO1YcvODvYRTK6sPm5gmQfyilkJWb5/lAIqK/KQbR17FHv91ilwV2FawmX83BwXOpuHH0Qny05BB+3ZqAvw5YArNtJy1B8X9WFJSRLN57zvA8RrS2f+8t2G+XUc7Ozce0tfHo+9FKvPzrTgBAtwl/4T8rCpZLf+xbS6byu3XxSDPIlrvy0+ZTmLUtAd3fX24Lms06csHyeu09k+zV44rL9pOXveqgMm/HGTw8bbMPR+Qf4+L22Vb/JP+YuuoYmr2xEBc5AZmIrlMMoq9zo0xkUi+kZGKgLkudllkQsJ5Pdi57GOlFdjbhsqXuevvJK1i4xzj4XmTdfvrKVXyw8KBt++7TlkD2XEomWo5ZhKjYOABAfr7CBwsP4Gyy65puzZ7TKbiYloWxv+9FTl7BRcSfu85i5PfO5QT+nkZ5+3/Woc+HKzB/d6mdf+uVnLx8u+4yZn21+jgmLjro+UAqVp8sOYShk1YDAOZsPw3A8vtHRHQ9YhB9nTt8wXNN7+Z4+xIMrbwDAJYdMFcqYFQ6AQDrjl60ffzkjG0uH+9Ntmv7qSv4z4qj6PLeX/hp00lk5ri+5fz+wgN4+899+HZdPBbpMuiTlh3G4n3n7Y69kpGNd/60bzBz4mIGLqZlIT0r1y4ILw5ZuXkux/6Um9fqWvKv33ahw7ilxf7akW98tuww9p5JAVCy5W1ERKURg2gqEd+uiwdgqav+eMkhxLy7xJY51jNqmZedl+9VCYM+GI6dvdtjxjI3z/KcSrmfPPrfVQW119qkzOy8fHSd8BdajFmEB7/ZZHqMZnR97y/c8ObCYj1naTN/jyWj7imI3nriEgZ9usrtBdEXy49g/9mUYh0feVaM83qJiK4pDKKpRKVn52HSssNISss23N/w9fm476sNTtvdreiod89/12PqKvuJhlqmGIDbTh5L9p3HofP2mfluE/7C2N/3un1ObYLn+mMFWfW9Z5LtgnkjefnKbjytxi7Cy7/sLBh3uv1rdDnd+DW7lmltIz0FYm/9sQ8HzqXi4LlUl8dMXHQQwyavcbn/bPJV/L6z1K7lVKrFvLvUqfTrWsxDK6WKtZsPEV3fGERTifCm9Z6+xEOjTS70ZKNDlw8AmLvjDFqMWYQLqZmYsdF50RmNUYB1+spVWxZdz9UKk7d9YVmgZuikNR47dzR6fT7+NWuX7fPUzFzM2paAlEz7ziAbjl1EVm4e+n680ukcuxKuYNCnq5CRbX5i5erDicjJy8cvm09hpsEiPCVJC8TchTXpWbnIs96hcHWcFhjl5Lk+011T1uO5mdtZOlIISWlZWOhwUajdtVEK6PPhCtz93/U+e/7s3HwcuWB/AXU1O8/rxZ4avGb/O3e9iIqNw/M/bfd8IBF5hUE0XTc6jluGN+bucdpuZmLUW3/sNXXbeoe124grV7Pz8Mbc3Ui1Bsq/bEkAUNClBAAys+1LFkZM3YC3/tiHSw6Z6KjYOAybvBYHzqVi+0n3zwtYApH+H6/EA99swsRFB/HqrF14bfZuZOfm24JUzYeLDiIqNq5QK1KeuJiOmHeX4syVq3hz7h70+XCF0zFXs/MwYcEBWxZfnx38YcMJ28cv/bIDLcYsstXhFiWLqK3maTaDuivhCoZOWm3XC93RvVM34L0F+13ud/TqbzttdzaUUthzumhdXnLz8m092/3peFK6U5vK4vTWH3vR7+NVOG/9XU2+moMbRy/Ep0uNF4pyR/udK62UUpiy8ijOGUzaLgpvEhklJS9f4YybRb2ISjsG0V765qEYfw+BipmZntzT1sa7XA3SnT2nk5GWlYtD5y1ZtB82nMAPG05isq4924ZjF7HioPtWe4fclDGYteLgBdtEUn3JS9M3FuDB/21E89EL8X/WjiRaK8HChKw/bjqJpLQsPDljG6ZvOIHjSelOC+pMXXUMU1YeRa5BhvnduILJm7O3nXb7XAmXM9D27cWId1PuczEtq1C38d/5cx/2nkmxdYExsv7YRfzXoU95fr7COhd3Kn7ZkmC7szFn+2nc/Pkal11pNFcysvHbVuPA7/U5uxH91mJk5eZ5NW+gONgy0T7uWXPmylXbHSStf/uVDMsF5dwdZ7Dz1BXb538Hx5LSMWHBAVMLbl3rJi46iK4T/ir2CwZ/S8/KxYK/SQclco9BtJfCy/h6pXT6Ozl1KQOPfbsZAz5ZheSrObbOJvpVIEdM3YBPPGTUtngI9I9cSMPMTScL3fpu7ZGLyMjOw5J95/HINNcTJCctO4zdCcm28pGo2DjcO3UDomLj8L81x+2O1WfX+360ElGxcYi13kp3XKRDWSssvlsXb1uYx4hjuPbrlgRczsjBL1tO2bbpV97cfzYF7d9dip82n7I9VgE4mpiGeTtOIyo2DnO3uw/UHa07koSo2DiXK21OWxeP+77eiLhdZ5GckYNcF+UjWv29pxU7X/x5B/756048/eM2RMXG2fVzn7vdkl3s/v5y9PlwBX7adLLEFkDRJtc6Xpv0/WiF23kE3l7QdHv/r4LHGgTst36xFvf8134eRWZOXqmbQ5Cfr3DaIOuakpljdxGg3RUyWinWW1viLxWqhaQZ51Myi9wvf80RS/KgsGOcsvIoomLj3N4t8iQtKxdRsXGYt+M0LqVn44npW4p8dyd29m48OWMbDpzjROe/OwbRXqoQGuzvIdA15ND5NFud9k26soYfNriuRe44fhlGz3MuO3FnzO978drs3XhqxjakW/8o3P6ftbb96Vm5pluSLT+YCK2K4/edp+1WPPx4ySHcMnkNmo9eZCsv0SZUvm1t/+dYjqL30+ZThtu14GiMh0mc4+PsF+X5bNlhAMDJSwWZ6MGfrbZ9rN0BWHskyS7Y6/vRSjz/0w4AwGuzd7t9zuSrOfjfmuO259Vq512VL2hB/NM/bkP024vxyLfGi92Y7RB3PsUSYMTtslwgZRoEyVoQEjt7N5q9sRB3/GetYfeb4uRq/EcT0/HtulumRboAACAASURBVHiXdfoNXpuP275YC6UUZm46ibhdZ20/Y3n5ylZCdOpSBo4lphmWUTluO3je/k7NA99sRNt3lnj3BRWjrScuY852+7sHX648im4T/sIxh4um1mMXo83bxmOdtOwwmo8ufIee4VPWu51s60l+vsKpSxm4kpHtdDHYbcJfGDqp8OcGgMAASwiSm+9+nkLy1RzDCy/twr0oQe8G6xycL5YfwX9XHsWivefdzhVZcfACNh5znrejd8r6fpSeVbjgPjs3H0lpWcjMyXOaI1OaXXAojVy45yz6fLjCqVzQLKWUbUG30opBtJea167g7yHQNUSfYXbstuHO9+tPeD7IhQXW8gB9nXSLMYu8mnyoefHnnXh42mbD2ug3XQT633kYe1RsnFNGrs3bS2wBrztbTlw2zHIm6TJZVw3a4OkvIBz/Fjser5TC7oSCDNsT07fg7T/32WW7jc7jyurDSU6vfVpWruna7H1u2vZlu8hyb/NQI5+UloXYWbtMZa09/RF39TL0mrgCr83ehcycPNz31QZsPHYRj39nKRfamZCMFYcS8drs3Xj6x214eNpmrDuShEavz0fD1+cDAHp8sBw3fWQ/mdbxNXcVyDv2tp+56STe+sP9BZr98yh8sfwI9p0pXCbxzi/X4cWfd+LAuRRExcbh4LlUrLWW+Jz1onTh4yWHkFHILKt2x8LM8ymlMH/3WadJt1+uPIoeHyxHm7eX4JXf7Cdk5hYyMDqfkomeHyzHlJVHkWb92fpmzXGXK+aeTb6K6LcWO3VdAgp+9grbsvzUpQw8bi1hU8pc+drD0zbjnqkbcD4lE3O2J7id1+DtuM4mX0VWbh6em7kdMe8uxQ1vLkTrsYsx6NNV+HWLcQKiKJRStjuG8Unptvfg7Scv2yVP9K5m5xm+b6w+nIiO45fZdaV69bddOJ6UbrdAmzd+2nwKd/xnnceSN39iEF0I0x/r6O8hUClW1MliRXXqknF9sJZ5LS5aZlTv/YUHCv3YAZ+sMjjSmVGQ7qp/tPaGnq170z/vYSLpL1tO4ZbJa2yBmBYr/GvWblxIySzUH+zmoxfh33MKMt4txyyynSc/X2H5wQu2C5VL6dkYF7cPA128Hgt2n0VWbh76GXRrcfT5ssNIvpqDlYcst83PJl/F1FVHMT5uP37afArzdpyxWwhp28nLTqUqJw3qzW/7Yq0t6MlwUXaQmJqFmZtOYVzcfqw7ehEv/bITS/cXLGCU4ZClu+/rjbaPlzosdKTxdOFyPCndsLXka7N3Y9raePcP1llzJAkTFx3EkEmrPR/sxnzrz/mCPQU/766+BsdMqzch6uX0bKeSiM3x5id6Ltt/AU/N2IbPrXd2NPpuR3N3FJQ9uXqP+21rgsuFtWzn2X4aJy9lYMKCAziaaLlr8+eus/hyhfGcE+3rMuqepL1k2q/ksMlr8J11zsGpSxkeJ0brJ5Xn68qMAkz8jt/71Qa8+PNO3Pz5GlM13e7GkpuXj07jl6LLe3/h2R+3O3XCOXAuFa/8tstt7f/aI0lY4uL3xpXpG07g1i/WYsXBC+j94QoM+GQVek1cjtv/sw4PTzO+g3bj6IXo9cEKp+27rIkH/eR6by+zdiVcsctmH7XO4Tl5qWTne3iDQXQh9GgS4e8hUCl28+dFu8VZVJ85/CEsDkcT00zdVnT1h9BRYbNYruxMsP+jHhUbh24T/sL83ZY/Rov2Fvxx6fHBcrfncsxi6ulvLb6uC4o/WlywoI+rjJpje0Wtpvj3nWfwyLTNmL7hBDJz8tDunSX4avVxpxIFzb9mWUo2jphYbfSjJYcQ/dZiPPS/TbiUno1R07di/PwDOH7R8kfpvfn70f395bZAZZ61Pnz1IfcTXXecumKbpKpNRgVgWP+t/dw43n04fcV1sHUm2bhjw9YTl9D+nSV25Tt6fT5cYdda0rGjjea3rQmIio2z3TL/du1xW5Dz4s878ICLhZP2nklGUloW8vKVVxM5BWLYunODrizgxyK0m2z7zhJ0GLcUgGW+gbu7Tpk5eXaLEl3NzrNlY884BIP6WFILWJMzcrBsv3OWMicvH//8dSfunrIeC/ectdUp705IxmJdUOjqItRVSUZwoLXkI08hKzcPLccsQtyus8jMyUOSdSVb7d1kV0Iyxvy+FysOXkCPD5bjnbh9hufUzN5WUHKjVMEFc06eQkpmDlIzcxAVG4dZBhN79V1F0rJy3NZ1x+06i4avz8eivecw6NNVTsemZ+XZyrYcV8rV04L+CymZThdd//h6o93vomb14USn+Soare++PvGiX5PBVU23mY5W7/65D6laBtrERcmC3WcxbPJau1au+jaapRWDaCLyqP8nq9B67GJ/D8MrRpO43ElMzcKo6VtddsIAgFE/bMPMTc63VT//6wgW7D6LTccv4WeTt121PxDa7fa/Dlxw+gMaX4wdN75fH2+72NBKfS5bu10Upu7SNgFOV25g1FPdVWu18fNd37Vw9Tf3zXl7cTG9oFuJ/g++UaYv1kVP6J+sAesbc/bgmR+3Yewf+/DHLss457iYaJqfrzB00hrEvLsU7d9dgj4frsDWE/bZ3py8fMzTZWy1EenLuj5desgWAI2YWjAh8t9z9ri8bX3ofCqiYuOwcI/9HZz4pHSnetPeE1eg+ehFhoHH1hOX8Pqc3Rj82WpcSLX83K06XHDB5Pi6GwW8D03bZDgRWhvHmeRMjPphG56z9qW+ZfIajJy+1WNdrDYvQilll43Uj2HtkSSkZeXi6R+34UJKQSDaafwyuwy4lkX1dPdB/7usUBCsTVx0EK3HLsar1hIWo1IS0b1a83acQYdxS20X2Y5f6dM/bgMAPDF9Kw6cS3XKqke/bf69dVfCFXQcvwy/mmzV+MA3m2zzVRxpq/W6+t4M+tT7OzHaa/i1LnDXfw+Tr1ouTPQT2E9cTMeTMyyvUWpmLpIzcnDmylV8tdpyjlIcQzOIJiJqNWYROoxb6nQb1RtPztiGj5e4X2JeT8vSpFnLIVYeSsRzM+0XxHBVd14Yny51fYfiWGI6Zm46acvCL9hzDlm5eTifkon4i+YD+ZLqWW30R1WrpdZzVU+sveYL957DUmtW9fmfdrgdv74ntdZq784v7ReYmbLiqF3ZlFEgu+XEZZcdWUb9sNU2Zv2dBq3UKW53wc/nT5tOoveHK2yBnuVr2O62BvrOL9djm7XTT3pWHpRSdq3Y1jtMmAtwiKLn7z7rskbfMdB0LC2YsfEEEi5nuLx4mrY2Hlm5eWjw2nx0HL/MVkOuSc/OxaPfFmRae060v6N02MWdGVddWhzLupRSTt1ftPklCgofLjqIr1c7B9MAsPGY5WJqxoaTmLHxBLKs5WWuysw0aVm5Xtf7al199Hcx9Bljfb2yp5Ie7YJ/7B/uM/bu5Od77rbzy+ZTdhOGAcsEdu33wHFeSvTbizFscsHE+NKciWa/NiK67qUWQzsxoCC4MsMoa+to9WHjftPFzfE28Mbjl9DsDe+7Qnjq613S1hxJQuuxi5y2H3DRdz36LeeM4I8bT+KPnWecAkwjZx0CM6NJrgAw8NPVODp+iOE+bdVTI3/sPINzyVcxokM9TFxkuWCbpStJ0Gf9zfTvXrzvPObqHpNw2XL3JjMnDze86fz9f8qaLTTy8RLn7LS+5GXhnnMYPc/95M5nfiy4iHxu5nZsfbO/09hc2eDi+6N1aYmfMBQXUjOx/2wqejWNQKfxy+yOi7+Y4TJYy87Nt+vtD9h/bzdZg9VZ2xLsvh/3fbURRrQJnI99u9lwlV1X/tx51jaO4xfT0XHcUgQFiF0ZTu+JK7D+tb4ALKu0+lrD1+djaOtaaF7L0nRhysqjuKFmebtj3o3bj0phIRjePtLuYstda0KtVAfwfS/6ovBpEC0i8QBSAeQByFVKxTjs7w1gHgAt7z9bKfW2L8dUXGpUKGOrYSIiAlwHZ9cLb0toXHnTQ7DljZRCdgbQ6GvfjTw1Yyvm7z6HJ3o2dOrm8Y2LWtS8fFXoFoSb4y9jc/xlVA0PcXvcgbPufxaPJ6XhTYMVXC+kZqL/x+Ym+XqiX63UqCbckbcT4/QcFz0ycteU9ThxMQMr/tnbcL+rjKq7xZwKY8KCA6gSHuJVAA3ALpB3tUqtqzsRW09cxp1frkO1ciGY8Xhnr9uouhO366wtiAaAF352nsS+4uAFDG8fiTUOdxjik9Jd1mxrSnMmWoqyjK7Hk1uC6BillGE6xRpE/1MpdbPZc8bExKgtW5yL50vahdRMdBy3zPOBRERE14joyIpOE3U9WfZyL/T9yHO3mMKY+3Q3t9l5cvZEz4bYcOyi3fexftUwuzkEZv00sjM6N6yK/WdTEBwo6Ge9wPrknmj8sfMs/jpg3ArPjF9HdTGVLX9lYDM83adxoZ+nqERkq2MSWMNyjkKqXj7U30MgIiIqVt4G0AB8FkADrtsdkmv/NZgIWZgAGrBMfo17rrvTwjov/ryzUOfTK4lyE1/z9cRCBWCxiGwVkZEujukiIjtFZIGItDA6QERGisgWEdmSmOi+9VJJalqjHAa1qInFL/b091CIiIj+dhxrkankfb6M3wNXfJ2J7qaUOiMi1QEsEZEDSil9sdU2APWVUmkiMgTAXABNHE+ilJoKYCpgKefw8ZhNW/xiL38PgYiIiMhnitK1qDiUVNefwvBpJlopdcb6/wUAcwB0dNifopRKs348H0CwiFTz5ZiIiIiI6NowddUx/LDBeaXa0sBnQbSIhItIee1jAAMA7HE4pqaIpRGliHS0jsfzFF4iIiIiui68YdBJpjTwZSa6BoA1IrITwCYAcUqphSIySkRGWY8ZDmCP9ZhJAEYoX7YLKQH/6FQPIdZlSp/x42xSIiIiIvIdn9VEK6WOAYg22D5F9/FkAJN9NQZ/ePvWlvjFugrQ8PaRnBRBRERE9DfEZb+LWWCAoGxwIACgeoUyqFGhDO5sF+nnURERERFRcWIQ7QOzn+qG14fcgLCQIGx8vR8GtKhh23ejblWfX0d1wRM9G/pjiERERETXjOSM0telg4ut+EDj6uXQuHo52+dajfQt0bVRr0pZ7D9rWRq2Q1QVdIiqYtgYHQCqhofgYnq27wdMRERERF5hJroE9GoagZf7N8W7t7b06nHX9AxLIiIiomKSkln6MtEMoktAQIDg2b5NUDEsGI90a4Bujati25v9vTrHx3c7zdF0svQlLv5CREREfz+pmbn+HoITBtHFpH39yqhevozH46qVK4MZj3dGlfAQ27bfn+lm+zh+wlDbx9Mf64geTarh4a5RuL1tHdv2uOe64+k+jezO26R6OTSKCMcrA5sV5csgIiIiIhNYE11MZj3ZtdCPbR1ZCX880x1HE9MAAO/c2gKzt59Gi9oVMf2xTrbjWtSugL1nUtCidkW0qF0RV7Pz8b+1xwEAPzzeCSKCp/s0xvT1J3AuJROAJSiPio2ze75+N1ZH3SphmLY2vtBjJiIiIiopIUHi7yE4YSa6lGgVWRG3WbPND3SJwpynujkdM3NkZyx6oaft88phwYbn2vB6X+x9ayA2/7uf4f6vH+qA8qHOj707JhI7RhuXmcTUr+zxawCAo+OHuNx3W5va6Nk0wtR5iIiIiDQBwiCaiqBCaDCa1Sxv+1w/8dDxRyu8TBAi3JSXGP0otqhdEZXCQuy2aS35fh3VxamExEhggODFfk0N9z3avQG+f7Sjx3MQERER6QmDaCpOlXSZ6KrlXAfMfzzT3Wmb2Z/F+c91x7HxQyAiuMPkojHP92uC1pEVzT2BD1RykaEnIiIiKi4Moq9h/+hUH+Nub4kj4wYjMMB1VNwqsiKevakxAgS21RPN3hYREQRYz90oohziJwxFxwZVDI9tVqMgSz790U64O6b4Vmp0l1XXrH61Dxa+0AM9mrBkhIiIiHyLQfQ1LDBA8I9O9REU6Pnb+PKAZjj23lB8ZG2Vpw+hb4muDQDo06w6ACB28A1uz/XLE13w7m3OPa/nPl1Qx10xLBgfDDduy7ftzf7o3NASiNepVNZu33t3tHI6/tmbGiPYzUWCpm6VMNxQswKUKih0uas9l1wnIiKi4scg+jqlZZdHdKiLSSPaIH7CUNSrGgagYMJi7YqhLh8/sEVNp21lQwJNPXeV8BBMdBFg16zg/JyCgvrv2ME3INzD8+hrxZ+9qYnt44Uv9DA1PiIiIipdSl9FNIPo65ZWkjG0dS2nYv3wMpbOhzdbM9RGqoaH4Pa2dTDtkQ6mnzMooODHTXtKfdYYMC7biKwcZltGfXj7SFsA//sz3bD1jX5497aWGHtLc9vxnXTlJiLAXy/3wvrXbsINNSvYtr+my7a/1N94IiQRERGRK+wTfZ3qEFUFB94ZhNBg56zukJa18M6t2bgrpq7LxwcECD65p43H55n/XA9sjr+EzJw83FiroGbaaJbtd492RMs6BRMSV7/aB0v2ncddMZEY1Komdp66gmrlymD8Ha3wUNcotI6sBAC4v3N9u/M80Lk+Rs/ba/u8YUQ5t2Ps06w6Pl5yyOPXUpym3N8eo37YWqLPSURERMWHQfR1zCiABiwB8gNdoorlOZrXroDmtSt4PhBAL4ce0nWrhOHR7g0AWNr7aRMGQ4MDEV23ksvziAjqVCqL01euOnUhqVOpLLo2qmq3raaLspVpj3RAw2rh6DVxhanxe6Nx9fBiPycREdHfVSnscMdyDvIvBWDB8z3w66guxXtehzIRzdrYmzDxLvt6bAXjY7s0rIrIymG2SZCuzH+uoNa6343VTY2vTJC5+nHNoBY1MUZXsqLZ8kY/rHqlj8fHd2lY1eMx3mhey9yFkV6UteaeiIjo78CnQbSIxIvIbhHZISJbDPaLiEwSkSMisktE2vlyPOQbL/VvateZwwz9BeWNtSqgQ5T7QLWwTDVnN46hAVg6oPw00n2Ar8+06zuS/KBbst1R3SpheGtYC5f7y5exv0n0yT1tnNoSNq9VAdXKlbFNCHVn5sjOto83vt7X4/Ge/O9h87XwI3s2xHM3NXb6XlQI9d2NsN+f8e7nkYiISjcphVMLSyIT3Ucp1UYpFWOwbzCAJtZ/IwF8WQLjoWL2XN8maOOmvMIdo4Txh3dF4+O7jbt3mPXGzc1RKSwY1cqFGO4fbqL1XbCudeBd7SMx/vZWHrOpVcILnq97k2r4dVQX3B0TifgJQzH+dvv2fbc4TNzUf81lggPs+m4DzreyftFl799xaDmofT/mPNUVO0cPsNtXo0Io9rw1ELvHDsC2NwuWeZ9yf3u3X5v9OTz37QaA8JBAPN27MV4a0MypfMjNtUuR1apY1vNB16CeTZ17oJcr493FyMNdo4ppNERE1zd/l3PcCuB7ZbEBQCURqeXnMVEJsHXnMAilhrePNL06oitDWtXCjtEDXJZNVC1XBh8Mbw3A0tO6UlgwyuqCvPgJQ+0WsJl4VzTu61QPK17pg1cHNXM639cPxmBUL+dl0TtEVbFlp+/rVM9unz4mnnJ/O9zRLtKuDd+tbe2DbC1Y6tk0Ao90i7Jr9fdA5/p2AX7ryIqInzAUbetVRkVry0J9T+5yZYJQPjTYLugf1NK5baHm2PghWBd7E+7rVA9Hxg2GiKBRhH1dtz6Q/9egG1A2OBDbRw+wPf+oXg3tjne8gCoTZHk7cqyNd+ehLvU9HwRg0r1t7T4Ps752jhcynozo4Hqyreb2tnWctjWt4X5yKwDE1K/s8Zjh7SPx+hD7Pu5jh7VAcKD5DM3zfZt4PqiI7u1Yz/NBXoiOrOixtWVpFh1ZEZGVC37/vPl+kTOjVXiJ/MHXQbQCsFhEtorISIP9dQCc0n2eYN1mR0RGisgWEdmSmJjoo6FSSSoNt2XujqmL+AlDUSYoEJv/3Q+7xg7w/CAX+jWv4XGRGkfahUSF0CAMamm5dqwaXpDhfdIhKL+1TR2Mvrk5pj7QHmNuaeGyVCUwQAwDpUUv9sQmF6Uc2jLtXz8Yg4plnZdNDwgQ1K5UFuNvb2Vb3Ocea0D5cNcojLu9Je7rWA+rXumDHx7rhCd7N8L+dwYhJKjgLcbxguaVgc1wR9s6+FbXJjF+wlB892hHRFvHc4dBQAoUBLOVw+3vNNzf2RK8hYUEIsQ6zpoVQjFMFyyHBAZg25v98XL/pi7veFQrF4ID7wyyXTDd1T4S3zwUgxtN1IJ/MLy103k/vacgiG9Xz/6uzdcPOt+kc+yXPueprqhTqSx6NK6GkT0b4eC7g2z7hrePxK4xA7H/7UF4oV8TvH9nKzx7U2O7x+uPF4Hh9zjIetFo9sJEY3y3p2j3GZ5zGP+8Z7rb3XnxlaUv9fTNiUXQpHrBhVRxvf+FmFhoC7Cft1HcHH+eryXe3H0j/7seJxZ2U0q1g6Vs42kRcXyHMnpJnN59lVJTlVIxSqmYiAgu6fx3UCU8BOXKBOHfQ50ny/lDcGCAXfmGO/1urAHAEthsfaOf0/55T3fD0pd6eTyPVt4wpFXBzRctkKlZMRQigtBgy5gUFAIDBI92b+Cyq4pm6Uu9ULWcc7lFuTJBqG6wmM3iF3vih8ct9dv9mtcwnUXUAgFt5czAAEG9qmHo3qSa4fHdm1RDqzoVsfjFnoifMBQPdY3Cx/e0Qa+mEbizXaRtDABQx5q163tjDbsJle3qVcJ3j3bEE70aIShAcGubgiB7x+j+GHtLC2x9ox/CywTZgvMKZS0Z/BtqWspj5j3TDaHBgXi2bxOX3/M72kUiNDjQli2vVr4M+t5Yw+WEVb3gwAC7OynDomsjvIzle6a/qACARS/0tGXq9f76p/3PT9t6lbE29ianiwZN2ZBAlA0JxAv9muKeDvXwYj/73uf6C5igwADsHDPAqWb/q4diED9hKN66taVdH3VHXz0Yg6kPWIKPezvWMyzJMvEy2Rna2vI7MP72Vphyf3u8NKDgbo9RoLhdV4YE2JdRufLG0Bs9HtO4enmPxxSGwP5uSAWDi5jC0J+nvptSs+a1KzjdwSguTQxes+K+E+ErJhbC9coTDnfbipvjHTW9DlGe72R58ssTvr9Q9cTojlPHqCp4dVAzu7s5pYVPg2il1Bnr/xcAzAHQ0eGQBAD6+6ORAM74ckxUOoQEBWDPWwPtMoSlwfrXbnKZrdU0rVHeViphFKxG161kWxzGndDgQGyxLhajqRwegs9GtMG0hy2/KiWRsW9aozwqhBb8MX66j30G3KgOFwB6NLUEy0arVxopVyYIfzzbHU2dar0FH90dbTe5ND9f22d/pd2nWXX0ahqBBtXCcWT8EDSoVlBSUiksBEGBAbbvidH3xoyywYGIHWQJOLSgWRuDY2yoL9FpW68SbmtT8PO8+tU+2PZmf0y6t61tAaOBLWraPracTxkGnGEhQRjZ0/MfZFdlAQEBgjX/6oNqutdg+5v9Me2RDrayIFcXOwDwRK9G+PNZ51vm0x/riP7Na2BAi5qY/lhHvDWshdNrMuPxTqZ+/jWT7m2Lj+6Kxp/Pdsd9neo5lRVp59IueDpEVUbl8BA0jHDdJrKtLju66fW+iJ8wFI/3aOg0YRcAXujXxO5/Xymv+x2rU7ks5j3dze4OgRkPd41ymuOgKRPk/s/5yJ4Fv9c/6yYa+8J7d7TyfBDgdLFnxozHXU/YdqW4OwM93r0BHjS4Y9OidkWDoy2e6u1c7ueND4a3dvv3spqb97uvDO52GenYwPsJ/o6/U+7mR5m5kHXUv3kNTH2wPZ7q7Tw5vTTwWRAtIuEiUl77GMAAAHscDvsdwIPWLh2dASQrpc76akxEntSqWNYwW+sr1cqVsZVHaG5tU8e2cuMD1jdqM1ly7RZ9YBHfaPR/7AEgzEXm+4aaFRA/YWih3ng90WrlA6Sgw0p03Up4qk9jp2Of79vE8I+2Y7292exoWEggAqwpqi7WnuJaj/J8h3PoJ4vOeaobPh1RkCmqWyXMliGtVq4M/nimOyYOb42P7zZepMjx2/avQTdg7tPdEPecczBr5uIqsnIYVrzSGxtes1wUVg4PQZ9m9i0YJ93b1mWtccs6FbFbV+L0+b1tba8DYHlNQoICUFWXBT48bjC6Na6GR7s1wKwnu7itY9aCvgHNayA0ONBuoSW9V6wlNU2ql8Nrg2/AF/dZmjjVrVwQGA1sUROf3BNtu0vUWLfAkv73eas1g/3lPyzniB18A17o1xTxE4biBRcBXS0XfeQdGWXMta9RCyy+f7QgjxRdt5LXrS57N4twuHNh+YH845nuXmX/O+laXpqpkf/u0Y52k5A/12VEKxncSTF8Tuv7RK+mEXjvjlYYe0tzPN+vCXo3M75If6JXQ9x0g3PL0G6NXV/8OdK+J18/FIPN/3a+a9isZnmnuxpm9Gtew25s2oWpPsitW8U+a9q2XuEzxdF1K+FuN4ufAZb3N6M5HpGVy6J/8xrY/mZ/3Nza/JSzNf/q47SmAuB8gaTvDhU/YajXnbo8+eeAZqgU5vlOk7/4crGVGgDmWP8ABgH4USm1UERGAYBSagqA+QCGADgCIAPAIz4cD5HfrXylt1PQ7M5rg2/AqwObmXrM1AdjMH/3WVMt70q7/s1rYtHe82haozwupGYBAFrXqWg32VPzoodl273N5utjkfb1q+DIuMG2199Tts+dVtY679DgQDSrUR4Hz6faBT6OQVBggLjM6rjqbe6oXJkgt907hkXXxqytCVh5yHiuSfnQYMRPGOr2Ob5/rCNWHkzEPR3q2i54AgIE7etXweY3+iFfAbsTknHvVxvsHmebXOzhS9ECfxHBE7p5Ap+NaIPlBy+gR5MIVCobjKDAAGTn5mPp/vMuaydDggJsX8+BdwaZ+n62jqyImKgq+GOn65uk8ROGYuT3W7B433m77fWrhuHju9ugmbWUqJyHto4/j+yMe6bav05/PtsdN3++BoDrlp01Kpq76/JItyhMWxtvt+3F/k0xrE1t7Dh5BSFBAXh2wbF+lgAAEZVJREFU5nYAllr3pLRsAAWTfcuHBqFhtXDbRT5guThbF3sTqpcvg8b/XuDyud++tSVqVwpFaHCgXVLg03ss38cm1ctj7ZEk5OTl48PFlhVk3xrWAmEhgfhzlyW3pl1QBgd5/p3+/ZluuLFWBdtz5TlcAd8TUxf1q9rfzXi4axS+XWf/+mha1qmAPadTAFjuTEVZH/tk70Z4pk9j5FrPXyE0CCmZufj6wQ5YeyQJ644mYen+C24vKEf2bIipq44BsPT+j3l3qd1+owyudkeuea0KeHXWLnRtXBWb4y87HTf7qa4ALN+nD++Ktr2WnkRWDkNYiPPP6x3t6uC12bttn0dVC8feMyl2x4QEBiA7L996nrJIuHzV7XMFBgjy8hWqhocgMyfPbp/Z9zp/8VkmWil1TCkVbf3XQik1zrp9ijWAhrUrx9NKqUZKqVZKKade0kR/J/Wrhtt1yfBEREwH3TUqhOKRbg0KOzQ3Yyj2U3o0vH0kDrwzCA0jyqF30+q2bd7Quq3U8bKObrBDOYH+9b+nQ91C3YJ29Gj3KABA7Upl0b5+ZdzXqR4+ucc4Q+1OaZigW6tiWYzoWM8wwAsLcR3Ez3qyK57s3chW9++tSmEhuL1tpN3dnAbVLBnoVpGeJ7uFBgeauj38wfBofH5vW8RPGGrLpgKWFU31Jt3bFm/ebKnf1ya4KmXJ6GuBnNGzjdXV/HfysCiS9vjyBsH427e2dNrmaMwtLQwvihpFlMOd7SPtMpVb3nDO0O4eOxDznulu9zpUKhuM2pXKmnqfKh8a7HRXTfs+tqxTEU/0aoTAgIL9dauEYfJ9BctHaOUSju0/9RY83wNf/qMdWkdWsnsux9c+wOCCfKxD7/5XBlrugrx7W0s806cgYy8iiKoWjm1v9serA5shvEyQ02TdmhVC8Wj3BvhsRFu8d0crdGlUFT/+XydMvs++rvnju6Pt5iAYlWUY1W5/92hHjB3WAnd3qItNr/fFA52dy0vKBgeievmCOymO82neudV4rYI72lnmmhh1I9K/5zzTpzE+uLM1BrWoibt078/zny+YyFopLBhb3+iHX0d1cXnBPGlEW+x5ayDWxt7k9Dvp7fyKksZlv4nIye/PdMPnfx3BEofMWknS3vDrVQ3zmA01Ur9qOL78Rzt0dbj96xg3ff9oR/yx8wx+3ZqAh7tGua3bCw4MwPP9muCTpYe8Ho/ePR3q4Z4OBfXUWlnIzP/rjIzs3CKduzTSskk31CyPYW1q4/7O9VEhNNhtDek3D8XY1Y+b0bFBFSx7uRcaVgtHj8bVDO9cmBUUIMjNV3atL797tCNaj12M7Lx8u24bgOXn9bHuDTCgeQ1k5uThhw0nkW8iAni4WwPUrxqOlMwcAAVZOQC20hWN9rO7NvYmZObk4YGvNyEpLRuBIujSqCqOvzcE//f9FjzZuxHu/HI9gKLVPy94vgcupWc7bRcRHBk3GLO2JeCu9gWB1sz/6+xykmdxXoyLCD65Jxov/rzT9rzanY7q5csYdtFxfH7HDjiaZS/3Qt+PVgKw1Nbvf3sQQoMDsGiv83uh0df676E3Inb2boRZJxOHlwmyTbTs2sjyXvTBwoM4eSnD7utxp1GE+zkGWslSTP3K+GPnGdzRrg4aVy/nsSe8Y+keALv32n7NayB+wlBExcbpxgo0igjH0cR09LmhOsLLBGHKA/ZdTho7dKKpWq4MqpYrg+0n7TPlI3s2RF6+woAWNZwuribc0QoHzqW6vWAqDRhEE5GT1pGV8P6drbHx2HLD/tfXisG6zicf3hWNj5ccdPqD1LNpBDo2qIKI8mXwXN8mXpXbAJYA2LH+sbC6GNQg/h1ot4Wb1iiPp3o717Ub6Wutb/aW9v2NquZ64qE70x7pgMTULLxuvWWtj29CgwPRuHo57DubgiBrxtRxEmXdKmE4ciHV8NxalrG9Q31sH1197dBWtfD7zjMYd3tLDG1dyxZcAwVZwAqhwagQGoxvH+2AFQcTbZNoRQRfP2SfIfeU3dZzDObctXQMCgywuxAE3P/8mo2h74qJxII9Zz0GgLe3jURgQACem7kdbetVQpXwEFxKz3Z581//tX1xXzsMbGH889Uoohz2vT0QcbvOokvDqrbH6b/PWktQI44XyEbmPNUV649dxG9bEzxOzA4JDLCrCf747miXP9sPdqmP3s0inMpU9GLqV8bgVrVw+HwqBreqiRd+dvv0TgJF0DqyEo4mpptadVb/I+V4TfnsTY0NA3kAGNK6FkZcA11eGEQTkaEq4SHYNXagv4dRbFpFVsS0RxwbBFmEBgfi1UGFawHmuIgOOWtTtxI+uisaA90s6FNaaDXYr+vqPvX+93AHLN1/HjUrhuLo+CFug0PHgK5ulTAsebGn2wD/5QFNcTb5qm2SWIXQYHRpWBXrj110yqbWqli21LaTW//aTbiYlo3nf9qOo4npph9XrVwZ/O6wmMq0RzoYTtwcFl3bNpmvQmgQLqVnewzWezSpZmupqHn/zlaoWLYgUA0LCcJdDhP5Glcvh51jBqBCaFCRu0RULVcGN7eujZtbG3fb+PSeNsjJy8crv+3C/Q5lGu4WIhMRtwE0APz2ZFevxxs/YSjy8xVy8xUCAgTjbm+JYdG10cREllj/Sml3F/s0i8AbNzc3DKD9X6DmHQbRRERe2vvW3+fiol/zGlh5KBENPPzxLao7vaxpL61qVgy1BTauykXqVQlHqzoV8foQ59IgT4FH/arh+HWUfaCjlcOURIBRr0oYnjHoguOtWhXLolbFsmhSvTyOJqYjzMvSHD3HrjJGpj/WCYv2nnPb2nLVK33sJkVqPGWONUaLFPnCbdZFpm5vW6dIJUlmTH2gPUZO3+rxuIAAQYh1LGEhQXZ3T9z558CCnu/3dqyH5Ks5GNmzocv1DqqVL4PUrNxrJphmEE1E5CVva3WLm1ZK8HiPok8kvb9TPdzWprbL26rXq5+f6Izftp62LYD0/+3deYwkdRnG8e/DLiwIeCAoCGQXCIoHioDIJeIREDDiFTVqIPEATzxCyCKJgf9Ag1FjovGKF4fxVoxXBCReLIK7yxJEF0FFkIUgiEeQ4/WP/g30DjM9U20PPcf3k3S6+tdVNdXPvOl5p7q6qoutlm/B96Y4z/awHvwYfISdxaF7PX7KQ5cuO+0Fo/shwLmveRZv/PPKTl+oHsbuOzyKtzxv8LnV5/OZi64562ju/M+9m411PbRsGEfN8jz/wzjhkJWbnRZzq+VbcMoMp1Q87y3P5ecbb18w70c20ZK0wCzbIkN92XIqSRbMH6xH0gErd+CAlaM/B/ownrLz9lx+wx2zujLjhNOP2YfvXz396czOf+vcXnBlwrYrlg+8sI96tl2xfOz/nI/SsO9PT3rsNjOeE3s+WTy/MUmSFqEzjnsqx+27C/vsPP0X/SY7+fl7bXZebUmjN/efFUiSpKGtWL6s01k2JD0y3BMtSZK0RJ3zqn0fdqpGzY5NtCRJ0hI127OT6OE8nEOSJEnqyCZakiRJ6sgmWpIkSerIJlqSJEnqyCZakiRJ6sgmWpIkSerIJlqSJEnqyCZakiRJ6ihVNe5t6CTJbcCfxvTjdwRuH9PPXsjMrTszG465DcfchmNuwzG34ZjbcP7f3FZW1U5TPbHgmuhxSvKbqjpw3Nux0Jhbd2Y2HHMbjrkNx9yGY27DMbfhzGVuHs4hSZIkdWQTLUmSJHVkE93Np8e9AQuUuXVnZsMxt+GY23DMbTjmNhxzG86c5eYx0ZIkSVJH7omWJEmSOrKJliRJkjqyiZ6FJC9Jcl2SjUlWj3t75oMkNya5OsnaJL9pYzsk+UmSP7T7x/XNf3rL77okR/eNH9DWszHJx5NkHK9nriT5fJJNSTb0jY0spyQrkny1jV+eZNUj+frmyjS5nZnkr63m1iY5tu+5JZ9bkt2TXJLk2iTXJHlPG7feBhiQm/U2QJKtk6xJsq7ldlYbt94GGJCb9TYLSZYl+W2Si9rj8dZbVXkbcAOWAdcDewJbAeuAp417u8Z9A24Edpw09iFgdZteDZzTpp/WclsB7NHyXNaeWwMcAgT4AXDMuF/biHM6Atgf2DAXOQHvAD7Vpl8HfHXcr3kOczsTOHWKec2t9zp2AfZv09sDv2/ZWG/D5Wa9Dc4twHZtekvgcuBg623o3Ky32eX3fuB84KL2eKz15p7omR0EbKyqP1bVf4ELgePHvE3z1fHAF9v0F4GX941fWFX3VNUNwEbgoCS7AI+uql9Vr2q/1LfMolBVlwF3TBoeZU796/o68KKJ/6oXsmlym465AVV1S1Vd1abvBq4FdsV6G2hAbtMxN6B6/tkebtluhfU20IDcpmNuTZLdgOOAz/YNj7XebKJntivwl77HNzH4DXapKODHSa5MclIbe2JV3QK9P0zAE9r4dBnu2qYnjy92o8zpwWWq6j7gLuDxc7bl4/euJOvTO9xj4mM7c5ukfQz5bHp7uay3WZqUG1hvA7WP1tcCm4CfVJX1NgvT5AbW20w+CpwGPNA3NtZ6s4me2VT/hXheQDisqvYHjgHemeSIAfNOl6HZbm6YnJZShp8E9gL2A24Bzm3j5tYnyXbAN4D3VtU/Bs06xZi5PZSb9TaDqrq/qvYDdqO3l+8ZA2Y3t2aa3Ky3AZK8FNhUVVfOdpEpxkaem030zG4Cdu97vBtw85i2Zd6oqpvb/SbgW/QOe7m1fVRCu9/UZp8uw5va9OTxxW6UOT24TJLlwGOY/WEQC0pV3dr++DwAfIZezYG5PSjJlvQawfOq6ptt2HqbwVS5WW+zV1V3ApcCL8F6m7X+3Ky3GR0GvCzJjfQOq31hkq8w5nqziZ7ZFcDeSfZIshW9g82/O+ZtGqsk2ybZfmIaOArYQC+XE9tsJwLfadPfBV7Xvvm6B7A3sKZ99HJ3koPbcUcn9C2zmI0yp/51vRq4uB3ntehMvFE2r6BXc2BuALTX+Dng2qr6SN9T1tsA0+VmvQ2WZKckj23T2wAvBn6H9TbQdLlZb4NV1elVtVtVraLXh11cVW9k3PVW8+DblvP9BhxL7xvb1wNnjHt7xn2jd6aSde12zUQm9I4d+inwh3a/Q98yZ7T8rqPvDBzAgfTeLK4HPkG7iuZiuQEX0Pto7l56/+W+eZQ5AVsDX6P3pYk1wJ7jfs1zmNuXgauB9e3Nbhdz2yyzw+l99LgeWNtux1pvQ+dmvQ3O7ZnAb1s+G4APtnHrbbjcrLfZZ3gkD52dY6z15mW/JUmSpI48nEOSJEnqyCZakiRJ6sgmWpIkSerIJlqSJEnqyCZakiRJ6sgmWpLmoST/bPerkrx+xOv+wKTHvxzl+iVpKbCJlqT5bRXQqYlOsmyGWTZroqvq0I7bJElLnk20JM1vZwPPS7I2yfuSLEvy4SRXJFmf5GSAJEcmuSTJ+fQu2kCSbye5Msk1SU5qY2cD27T1ndfGJvZ6p617Q5Krk7y2b92XJvl6kt8lOa9d7UuSlqzl494ASdJAq4FTq+qlAK0ZvquqnpNkBfCLJD9u8x4EPKOqbmiP31RVd7TLC1+R5BtVtTrJu6pqvyl+1iuB/YBnATu2ZS5rzz0beDpwM/AL4DDg56N/uZK0MLgnWpIWlqOAE5KsBS6nd9nbvdtza/oaaIBTkqwDfg3s3jffdA4HLqiq+6vqVuBnwHP61n1TVT1A79LYq0byaiRpgXJPtCQtLAHeXVU/2mwwORL416THLwYOqap/J7kU2HoW657OPX3T9+PfD0lLnHuiJWl+uxvYvu/xj4C3J9kSIMmTk2w7xXKPAf7eGuh9gIP7nrt3YvlJLgNe24673gk4AlgzklchSYuMexIkaX5bD9zXDsv4AvAxeodSXNW+3Hcb8PIplvsh8LYk64Hr6B3SMeHTwPokV1XVG/rGvwUcAqwDCjitqv7WmnBJUp9U1bi3QZIkSVpQPJxDkiRJ6sgmWpIkSerIJlqSJEnqyCZakiRJ6sgmWpIkSerIJlqSJEnqyCZakiRJ6uh/2XqnvhVctgUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function.\n",
    "plt.plot(word2vec_solver.loss_history)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to six: seven five four eight three zero nine two\n",
      "Nearest to eight: seven six five four nine three zero one\n",
      "Nearest to three: two four five six seven eight zero one\n",
      "Nearest to work: progress poetry friends writing discoveries wife trip period\n",
      "Nearest to friends: friend work feelings brother up functioning ideas lectures\n",
      "Nearest to king: duke prince iv maria albert emperor queen daughter\n"
     ]
    }
   ],
   "source": [
    "# Print the nearest words\n",
    "common_words = np.array([word_to_idx[\"six\"], word_to_idx[\"eight\"], word_to_idx[\"three\"],\n",
    "                         word_to_idx[\"work\"], word_to_idx[\"friends\"], word_to_idx[\"king\"]])\n",
    "similar_words = SkipGram.sample(common_words, top_k=8)\n",
    "\n",
    "for line in similar_words:\n",
    "    print(line)\n",
    "\n",
    "# for idx in range(len(common_words)):\n",
    "#     word_id = common_words[idx]\n",
    "#     print(\"\\nNearest to %s: \" %(idx_to_word[word_id]), end=\"\")\n",
    "#     for ids in similar_words[idx]:\n",
    "#         print(idx_to_word[ids], end=\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to 'king' - 'man' + 'woman': woman duke daughter prince preceded lf gregory maria "
     ]
    }
   ],
   "source": [
    "U = SkipGram.params[\"U\"]\n",
    "norm_U = U / np.linalg.norm(U, axis=1, keepdims=True)\n",
    "\n",
    "king = norm_U[word_to_idx[\"king\"]]\n",
    "man = norm_U[word_to_idx[\"man\"]]\n",
    "woman = norm_U[word_to_idx[\"woman\"]]\n",
    "\n",
    "top_k = 8\n",
    "vect = king - man + woman\n",
    "args = np.argsort(norm_U.dot(vect))[::-1]\n",
    "nearest = args[1:top_k + 1]\n",
    "\n",
    "print(\"Nearest to 'king' - 'man' + 'woman': \", end=\"\")\n",
    "for ids in nearest:\n",
    "    print(idx_to_word[ids], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "It should be noted that calculating the objective function is computationally huge. Any evaluation of the objective function would take $ O(|V|) $ time due to the summation over the entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch generation takes 0.01299 seconds\n",
      "Loss and grads computation takes 3.41987 seconds\n",
      "Parameter update takes 0.50515 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate batch.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    batch = dataset.train_batch(batch_size)\n",
    "toc = time.time()\n",
    "print(\"Batch generation takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the loss and gradients.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = word2vec_solver.model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Update the parameters.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    for p, w in word2vec_solver.model.params.items():\n",
    "        dw = grads[p]\n",
    "        config = word2vec_solver.optim_configs[p]\n",
    "        next_w, next_config = word2vec_solver.update_rule(w, dw, config)\n",
    "toc = time.time()\n",
    "print(\"Parameter update takes %.5f seconds\" % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the exp scores takes 1.51318 seconds\n",
      "Computing the loss takes 0.07601 seconds\n",
      "Computing the gradients step 1 takes 1.47902 seconds\n",
      "Computing the gradients step 2 takes 0.02099 seconds\n"
     ]
    }
   ],
   "source": [
    "N, span = batch.shape\n",
    "window_size = (span - 1) // 2\n",
    "\n",
    "window = np.concatenate((batch[:, : window_size], batch[:, window_size + 1 : ]), axis=1)\n",
    "target = batch[:, window_size].reshape(N, 1)\n",
    "\n",
    "N, T = window.shape\n",
    "labels = window.reshape(N * T)\n",
    "\n",
    "scores, caches = SkipGram._forward(target)\n",
    "scores = scores[:, np.newaxis, :]\n",
    "scores = np.tile(scores, (1, T, 1))\n",
    "scores = scores.reshape(N * T, -1)\n",
    "N, C = scores.shape\n",
    "\n",
    "shifted_scores = scores - np.max(scores, axis=1, keepdims=True)\n",
    "\n",
    "# Compute the exponents.\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    exp_scores = np.exp(shifted_scores)\n",
    "toc = time.time()\n",
    "print(\"Computing the exp scores takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Computhe the loss\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    Y = np.log(np.sum(exp_scores, axis=1, keepdims=True))\n",
    "    Z = shifted_scores - Y\n",
    "    loss = - np.sum(Z[np.arange(N), labels]) / N\n",
    "toc = time.time()\n",
    "print(\"Computing the loss takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the grads\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    dscores = np.exp(Z)\n",
    "toc = time.time()\n",
    "print(\"Computing the gradients step 1 takes %.5f seconds\" % (toc - tic))\n",
    "\n",
    "# Compute the grads\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    dscores[np.arange(N), labels] -= 1\n",
    "    dscores /= N\n",
    "toc = time.time()\n",
    "print(\"Computing the gradients step 2 takes %.5f seconds\" % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "In 2014 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeff Dean address the problem of effiiency in their paper:  \n",
    "<i>[2] \"Distributed Representations of Word Phrases and their Compositionality\"</i>.\n",
    "\n",
    "The proposed Negative-Sampling method is described in detail by Yoav Goldberg and Omer Levy in ther paper:  \n",
    "<i>[3] \"word2vec Explained: Deriving Mikolov et. al.'s Negative-Sampling Word-Embedding Method\"</i>.\n",
    "\n",
    "In the previous setting we considered the conditional probability $ P(w_{t} | w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j}) = P(w|c) $ of a word given its context (or similarly $ P(c, w) $) from a given corpus of text, and the goal was to maximize to corpus probability:\n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\prod_{t=1}^{T} P(w | c; \\eta) \\space - \\space \\text{for the CBOW algorithm} $$  \n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\displaystyle \\prod_{t=1}^{T} P(c | w; \\eta) \\space - \\space \\text{for the Skip-Gram algorithm} $$\n",
    "\n",
    "To do this we model the conditional probability $ P(w | c; \\eta) $ using softmax:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(w | c; \\eta) = \\frac{exp({u_{c}\\theta_{w}})}{\\displaystyle\\sum_{v \\in V}exp({u_{c}\\theta_{v}})} $$\n",
    "\n",
    "However, computing this objective is very expensive due to the summation over the entire vocabulary. A more efficient way of deriving word embeddings would be to consider a different objective. Consider a pair $ (w, c) $ of word and context and denote by $ P(D = 1| w, c) $ the probability that this pair comes from the corpus data. We can model that probability using a sigmoid function:\n",
    "\n",
    "$$ \\displaystyle\n",
    "P(D = 1|w,c; \\eta) = \\frac{1}{1 + exp( - u_{c} \\theta_{w})} $$\n",
    "\n",
    "In order for the model to learn we have to present some $ (w,c) $ pairs which are not in the data. Now, the new objective function tries to maximize the probability of a pair being in the corpus data if it inedeed is, and maximize the probability of a pair not being in the corpus data if it indeed is not.  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "L(\\eta) = \\prod_{(w,c) \\in D} P (D = 1|w,c;\\eta) \\prod_{(w, c) \\notin D} (1 - P(D = 1|w,c;\\eta)) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = - log L(\\eta) = - \\left( \\sum_{(w,c) \\in D} log \\frac{1}{1 + exp( - u_{c} \\theta_{w})} + \\sum_{(w,c) \\notin D} log \\left( 1 - \\frac{1}{1 + exp( - u_{c} \\theta_{w})} \\right) \\right) $$  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J(\\eta) = - \\left( \\sum_{(w,c) \\in D} log \\frac{1}{1 + exp( - u_{c} \\theta_{w})} + \\sum_{(w,c) \\notin D} log \\frac{1}{1 + exp(u_{c} \\theta_{w})} \\right) $$  \n",
    "\n",
    "\n",
    "For CBOW, our new objective function for observing the center word $ w_{t} $ given the context vector $ \\displaystyle \\hat{v} = \\frac{w_{t-j} + \\cdots + w_{t-1} + w_{t+1} + \\cdots + w_{t+j}}{2j} $ would be:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J_{CBOW}(\\eta) = - log \\frac{1}{1 + exp(-\\theta_{t} \\hat{v})} -  \\sum_{k=1}^{K} log \\frac{1}{1 + exp(\\theta_{k} \\hat{v})} $$  \n",
    "\n",
    "\n",
    "Thus, the negative examples consist of our input context words and \"untrue\" center words.\n",
    "\n",
    "For Skip-Gram, our new objective function for observing the context words $ w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j} $ given the center word $ w_{t} $ would be:  \n",
    "\n",
    "\n",
    "$$ \\displaystyle\n",
    "J_{SkipGram}(\\eta) = - \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} log \\frac{1}{1 + exp(-w_{t} \\theta_{t+m})} - \\sum_{k=1}^{K} log \\frac{1}{1 + exp(w_{t} \\theta_{k})} $$\n",
    "\n",
    "Thus, the negative examples consist of our input center word and \"untrue\" context words.\n",
    "\n",
    "The words $ \\{w_{k}\\}_{k=1}^{K} $ are sampled from a noise distribution. The authors of [2] propose sampling from a uniform distribution raised to the power of $ 3/4 $. The authors also propose working with values of $ K = 5...20 $.\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "\n",
    "For computing the second term of the objective functions, we could choose both the center word $ w_{t} $ and the context words $ w_{t-j}, ..., w_{t-1}, w_{t+1}, ..., w_{t+j} $ of the negative examples from our noisy distribution.  \n",
    "However, keeping the input and randomly sampling only the \"untrue\" output has an appealing intuitive property. Firstly, for a given input we will only update the weights of the softmax classifier for the \"true\" output and for the randomly chosen \"untrue\" outputs. And secondly, for a given input we will only update the weights of the embeddings of that input by showing the model the \"true\" output and the randomly choosen \"untrue\" outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "N = 5\n",
    "V = 8\n",
    "K = 3\n",
    "T = 2\n",
    "\n",
    "s = np.array([np.arange(V) * 0.01] * N) * np.arange(1, N+1).reshape(N, 1)\n",
    "s = s[:, np.newaxis, :]\n",
    "s = np.tile(s, (1, T, 1))\n",
    "s = s.reshape(N*T, -1)\n",
    "\n",
    "y = np.random.randint(V, size=(N, T))\n",
    "z = np.random.randint(V, size=(N, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 17.56620\n",
      "Loss = 17.56620\n"
     ]
    }
   ],
   "source": [
    "# Check the forward pass.\n",
    "loss = 0\n",
    "for _n in range(N):\n",
    "    for _t in range(T):\n",
    "        loss -= np.log(sigmoid(s[_n, y[_n, _t]]))\n",
    "    for _k in range(K):\n",
    "        loss -= np.log(sigmoid(-s[_n, z[_n, _k]]))\n",
    "\n",
    "print(\"Loss = %.5f\" % loss)\n",
    "\n",
    "loss, _ = negative_sampling_loss(s, y, z)\n",
    "print(\"Loss = %.5f\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max relative error: 3.865674e-10\n"
     ]
    }
   ],
   "source": [
    "# Check the backward pass.\n",
    "loss, ds = negative_sampling_loss(s, y, z)\n",
    "\n",
    "f = lambda x: negative_sampling_loss(x, y, z)[0]\n",
    "ds_num = eval_numerical_gradient(f, s, verbose=False)\n",
    "print(\"max relative error: %e\" % (rel_error(ds, ds_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and grads computation takes 3.40306 seconds.\n",
      "Loss and grads computation takes 0.48831 seconds with negative sampling.\n"
     ]
    }
   ],
   "source": [
    "# Compare the efficiency.\n",
    "np.random.seed(seed=0)\n",
    "model = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                 model_type=\"skipgram\",\n",
    "                 word_to_idx=word_to_idx,\n",
    "                 dtype=np.float32)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds.\" % (toc - tic))\n",
    "\n",
    "\n",
    "model = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                 model_type=\"skipgram\",\n",
    "                 negative_sampling=10,\n",
    "                 word_to_idx=word_to_idx,\n",
    "                 dtype=np.float32)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    loss, grads = model.loss(batch)\n",
    "toc = time.time()\n",
    "print(\"Loss and grads computation takes %.5f seconds with negative sampling.\" % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors using Negative Sampling\n",
    "\n",
    "Again, we will train word vectors using the Skip-Gram model on Wikipedia articles. However, this time we will use the negative sampling loss to evaluate our model.    \n",
    "Again, we will learn word vector representations for the most common words in the text and will replace rare words with `UNK` token.  \n",
    "The model uses L2 regularization of the weight matrices.  \n",
    "Parameter update is performed using Adam update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model using negative sampling.\n",
    "np.random.seed(seed=None)\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "num_negative_samples = 20\n",
    "\n",
    "SkipGram = word2vec(vocab_size=vocab_size, embed_size=embed_size,\n",
    "                    model_type=\"skipgram\",\n",
    "                    negative_sampling = num_negative_samples,   # use negative sampling\n",
    "                    word_to_idx=word_to_idx,\n",
    "                    dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations per epoch: 7812\n",
      "(Iteration 0 / 39060); Epoch(0 / 5); loss: 2128.58447\n",
      "Sample:\n",
      " Nearest to friend: preserved dawn artemis corner lives anti increased relations adjacent publications\n",
      "(Iteration 5000 / 39060); Epoch(0 / 5); loss: 812.27768\n",
      "Sample:\n",
      " Nearest to friend: seized regardless scored wheel cf deities removal ted astrology thereby\n",
      "(Iteration 10000 / 39060); Epoch(1 / 5); loss: 831.61373\n",
      "Sample:\n",
      " Nearest to friend: tucker logo proto cardinal sale resolution sort namely sri oral\n",
      "(Iteration 15000 / 39060); Epoch(1 / 5); loss: 730.70374\n",
      "Sample:\n",
      " Nearest to friend: sri aspect stress factions topics poll tells pairs seized donald\n",
      "(Iteration 20000 / 39060); Epoch(2 / 5); loss: 751.35374\n",
      "Sample:\n",
      " Nearest to friend: toxic development denote sri europe unions louisiana in engaged series\n",
      "(Iteration 25000 / 39060); Epoch(3 / 5); loss: 727.93465\n",
      "Sample:\n",
      " Nearest to friend: toxic sri UNK its suggesting louisiana traces the rally eight\n",
      "(Iteration 30000 / 39060); Epoch(3 / 5); loss: 714.38727\n",
      "Sample:\n",
      " Nearest to friend: toxic series one the sri denote five UNK on is\n",
      "(Iteration 35000 / 39060); Epoch(4 / 5); loss: 759.77226\n",
      "Sample:\n",
      " Nearest to friend: toxic that on treatments the as is suggesting a sri\n",
      "training took 87.082 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "word2vec_solver = UnsupervisedSolver(SkipGram, dataset,\n",
    "                                     update_rule=\"adam\",\n",
    "                                     optim_config={\"learning_rate\": 1e-2},\n",
    "                                     lr_decay=0.5,\n",
    "                                     batch_size=batch_size,\n",
    "                                     num_epochs=5,\n",
    "                                     print_every=5000,\n",
    "                                     verbose=True)\n",
    "\n",
    "tic = time.time()\n",
    "word2vec_solver.train()\n",
    "toc = time.time()\n",
    "print(\"training took %.3f minutes\" % ((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function.\n",
    "plt.plot(word2vec_solver.loss_history)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to six: of in UNK the a and one eight\n",
      "Nearest to eight: the UNK of and one in a to\n",
      "Nearest to three: the UNK of one and to in a\n",
      "Nearest to work: UNK on nine four three of the to\n",
      "Nearest to friends: had empty one UNK the detective that on\n",
      "Nearest to king: the UNK one on to for as in\n"
     ]
    }
   ],
   "source": [
    "# Print the nearest words\n",
    "common_words = np.array([word_to_idx[\"six\"], word_to_idx[\"eight\"], word_to_idx[\"three\"],\n",
    "                         word_to_idx[\"work\"], word_to_idx[\"friends\"], word_to_idx[\"king\"]])\n",
    "similar_words = SkipGram.sample(common_words, top_k=8)\n",
    "\n",
    "for line in similar_words:\n",
    "    print(line)\n",
    "\n",
    "# for idx in range(len(common_words)):\n",
    "#     word_id = common_words[idx]\n",
    "#     print(\"\\nNearest to %s: \" %(idx_to_word[word_id]), end=\"\")\n",
    "#     for ids in similar_words[idx]:\n",
    "#         print(idx_to_word[ids], end=\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to 'king' - 'man' + 'woman': king plateau maryland unfortunately unions denmark german referring "
     ]
    }
   ],
   "source": [
    "U = SkipGram.params[\"U\"]\n",
    "norm_U = U / np.linalg.norm(U, axis=1, keepdims=True)\n",
    "\n",
    "king = norm_U[word_to_idx[\"king\"]]\n",
    "man = norm_U[word_to_idx[\"man\"]]\n",
    "woman = norm_U[word_to_idx[\"woman\"]]\n",
    "\n",
    "top_k = 8\n",
    "vect = king - man + woman\n",
    "args = np.argsort(norm_U.dot(vect))[::-1]\n",
    "nearest = args[1:top_k + 1]\n",
    "\n",
    "print(\"Nearest to 'king' - 'man' + 'woman': \", end=\"\")\n",
    "for ids in nearest:\n",
    "    print(idx_to_word[ids], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
